#!/usr/bin/env python3
"""
Script de pruebas para LLM Service
TFM Vicente Caruncho - Sistemas Inteligentes

Ejecutar desde el directorio raÃ­z del proyecto:
python tests/test_llm_service.py
"""

import sys
import os
import json
import time
from pathlib import Path

# AÃ±adir el directorio raÃ­z al path
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.services.llm_service import get_llm_service, LLMRequest
from app.models import DocumentChunk, DocumentMetadata

def print_section(title: str):
    """Imprimir secciÃ³n con formato"""
    print(f"\n{'='*60}")
    print(f"ğŸ§ª {title}")
    print('='*60)

def test_service_initialization():
    """Test 1: InicializaciÃ³n del servicio"""
    print_section("INICIALIZACIÃ“N DEL SERVICIO")
    
    try:
        service = get_llm_service()
        print("âœ… Servicio LLM inicializado correctamente")
        
        # Verificar proveedores
        providers = list(service.providers.keys())
        print(f"ğŸ“‹ Proveedores registrados: {providers}")
        
        return service
    except Exception as e:
        print(f"âŒ Error inicializando servicio: {e}")
        return None

def test_provider_availability(service):
    """Test 2: Disponibilidad de proveedores"""
    print_section("DISPONIBILIDAD DE PROVEEDORES")
    
    try:
        availability = service.get_available_providers()
        
        for provider, available in availability.items():
            status = "âœ… DISPONIBLE" if available else "âŒ NO DISPONIBLE"
            print(f"{provider}: {status}")
        
        return availability
    except Exception as e:
        print(f"âŒ Error verificando disponibilidad: {e}")
        return {}

def test_available_models(service):
    """Test 3: Modelos disponibles"""
    print_section("MODELOS DISPONIBLES")
    
    try:
        models = service.get_available_models()
        
        for provider, model_list in models.items():
            print(f"\n{provider.upper()}:")
            if model_list:
                for model in model_list:
                    print(f"  - {model}")
            else:
                print("  No hay modelos disponibles")
        
        return models
    except Exception as e:
        print(f"âŒ Error obteniendo modelos: {e}")
        return {}

def test_simple_generation(service, availability):
    """Test 4: GeneraciÃ³n simple sin contexto"""
    print_section("GENERACIÃ“N SIMPLE SIN CONTEXTO")
    
    request = LLMRequest(
        query="Â¿QuÃ© es una licencia de obras municipal?",
        temperature=0.7,
        max_tokens=200
    )
    
    results = {}
    
    # Test Ollama
    if availability.get('ollama', False):
        print("\nğŸ¦™ Testing Ollama...")
        try:
            start_time = time.time()
            result = service.generate_response(request, 'ollama')
            test_time = time.time() - start_time
            
            print(f"âœ… Modelo: {result.model_name}")
            print(f"â±ï¸  Tiempo total: {test_time:.2f}s")
            print(f"â±ï¸  Tiempo generaciÃ³n: {result.response_time:.2f}s")
            print(f"ğŸª™ Tokens: {result.total_tokens}")
            print(f"ğŸ“ Respuesta (100 chars): {result.response[:100]}...")
            
            if result.error:
                print(f"âš ï¸  Error: {result.error}")
            
            results['ollama'] = result
            
        except Exception as e:
            print(f"âŒ Error en Ollama: {e}")
    
    # Test OpenAI
    if availability.get('openai', False):
        print("\nğŸ¤– Testing OpenAI...")
        try:
            start_time = time.time()
            result = service.generate_response(request, 'openai')
            test_time = time.time() - start_time
            
            print(f"âœ… Modelo: {result.model_name}")
            print(f"â±ï¸  Tiempo total: {test_time:.2f}s")
            print(f"â±ï¸  Tiempo generaciÃ³n: {result.response_time:.2f}s")
            print(f"ğŸª™ Tokens: {result.total_tokens}")
            print(f"ğŸ’° Coste estimado: ${result.estimated_cost:.4f}")
            print(f"ğŸ“ Respuesta (100 chars): {result.response[:100]}...")
            
            if result.error:
                print(f"âš ï¸  Error: {result.error}")
            
            results['openai'] = result
            
        except Exception as e:
            print(f"âŒ Error en OpenAI: {e}")
    
    return results

def test_rag_generation(service, availability):
    """Test 5: GeneraciÃ³n con contexto RAG"""
    print_section("GENERACIÃ“N CON CONTEXTO RAG")
    
    # Crear chunks de prueba simulando documentos municipales
    chunks = [
        DocumentChunk(
            content="""La licencia de obras es un acto administrativo por el que el Ayuntamiento 
            autoriza la realizaciÃ³n de obras de construcciÃ³n, instalaciÃ³n o demoliciÃ³n, 
            asÃ­ como el uso del suelo para dichos fines. Es obligatoria para obras mayores 
            que requieran proyecto tÃ©cnico.""",
            metadata=DocumentMetadata(
                source_path="ordenanza_urbanistica_2024.pdf",
                source_type="pdf",
                page_number=15,
                title="Ordenanza de Urbanismo - Licencias de Obra"
            )
        ),
        DocumentChunk(
            content="""Los plazos para resolver las solicitudes de licencia de obras son:
            - Obras menores: 15 dÃ­as hÃ¡biles desde presentaciÃ³n completa
            - Obras mayores: 30 dÃ­as hÃ¡biles desde presentaciÃ³n completa
            - Obras en suelo rÃºstico: 45 dÃ­as hÃ¡biles
            El silencio administrativo tendrÃ¡ efectos desestimatorios.""",
            metadata=DocumentMetadata(
                source_path="reglamento_tramitacion_2024.pdf",
                source_type="pdf",
                page_number=8,
                title="Reglamento de TramitaciÃ³n - Plazos"
            )
        )
    ]
    
    request = LLMRequest(
        query="Â¿CuÃ¡les son los plazos para obtener una licencia de obras?",
        context=chunks,
        temperature=0.3,  # MÃ¡s determinista para RAG
        max_tokens=300
    )
    
    results = {}
    
    # Test Ollama con RAG
    if availability.get('ollama', False):
        print("\nğŸ¦™ Testing Ollama con RAG...")
        try:
            result = service.generate_response(request, 'ollama')
            
            print(f"âœ… Modelo: {result.model_name}")
            print(f"â±ï¸  Tiempo: {result.response_time:.2f}s")
            print(f"ğŸ“š Fuentes: {result.sources}")
            print(f"ğŸ“ Respuesta: {result.response}")
            
            results['ollama_rag'] = result
            
        except Exception as e:
            print(f"âŒ Error en Ollama RAG: {e}")
    
    # Test OpenAI con RAG
    if availability.get('openai', False):
        print("\nğŸ¤– Testing OpenAI con RAG...")
        try:
            result = service.generate_response(request, 'openai')
            
            print(f"âœ… Modelo: {result.model_name}")
            print(f"â±ï¸  Tiempo: {result.response_time:.2f}s")
            print(f"ğŸ’° Coste: ${result.estimated_cost:.4f}")
            print(f"ğŸ“š Fuentes: {result.sources}")
            print(f"ğŸ“ Respuesta: {result.response}")
            
            results['openai_rag'] = result
            
        except Exception as e:
            print(f"âŒ Error en OpenAI RAG: {e}")
    
    return results

def test_model_comparison(service, availability):
    """Test 6: ComparaciÃ³n directa entre modelos"""
    print_section("COMPARACIÃ“N DIRECTA ENTRE MODELOS")
    
    if not (availability.get('ollama') and availability.get('openai')):
        print("âš ï¸  ComparaciÃ³n requiere ambos proveedores disponibles")
        return {}
    
    # Crear contexto de prueba
    chunks = [
        DocumentChunk(
            content="""El procedimiento de solicitud de licencia de obras requiere:
            1. Instancia de solicitud cumplimentada
            2. Proyecto tÃ©cnico visado (obras mayores)
            3. Justificante de pago de tasas municipales
            4. DocumentaciÃ³n catastral actualizada
            5. AutorizaciÃ³n de la comunidad (si procede)""",
            metadata=DocumentMetadata(
                source_path="guia_tramites_2024.pdf",
                source_type="pdf",
                title="GuÃ­a de TrÃ¡mites Municipales"
            )
        )
    ]
    
    request = LLMRequest(
        query="Â¿QuÃ© documentaciÃ³n necesito para solicitar una licencia de obras mayor?",
        context=chunks,
        temperature=0.3,
        max_tokens=250
    )
    
    print("ğŸš€ Ejecutando comparaciÃ³n en paralelo...")
    try:
        results = service.compare_models(request)
        
        print(f"\nğŸ“Š RESULTADOS DE COMPARACIÃ“N:")
        print("-" * 50)
        
        for provider, result in results.items():
            print(f"\n{provider.upper()}:")
            print(f"  Modelo: {result.model_name}")
            print(f"  Tiempo: {result.response_time:.2f}s")
            print(f"  Tokens: {result.total_tokens}")
            if result.estimated_cost:
                print(f"  Coste: ${result.estimated_cost:.4f}")
            print(f"  Respuesta: {result.response[:150]}...")
            if result.error:
                print(f"  Error: {result.error}")
        
        return results
        
    except Exception as e:
        print(f"âŒ Error en comparaciÃ³n: {e}")
        return {}

def test_service_stats(service):
    """Test 7: EstadÃ­sticas del servicio"""
    print_section("ESTADÃSTICAS DEL SERVICIO")
    
    try:
        stats = service.get_service_stats()
        print(json.dumps(stats, indent=2, ensure_ascii=False))
        return stats
    except Exception as e:
        print(f"âŒ Error obteniendo estadÃ­sticas: {e}")
        return {}

def test_error_handling(service):
    """Test 8: Manejo de errores"""
    print_section("MANEJO DE ERRORES")
    
    # Test proveedor inexistente
    print("ğŸ§ª Testing proveedor inexistente...")
    request = LLMRequest(query="Test", max_tokens=50)
    result = service.generate_response(request, 'proveedor_falso')
    print(f"Resultado: {result.error or 'Error no capturado'}")
    
    # Test modelo inexistente en Ollama
    print("\nğŸ§ª Testing modelo inexistente...")
    result = service.generate_response(request, 'ollama', 'modelo_inexistente')
    print(f"Resultado: {result.response[:100]}...")

def generate_test_report(all_results):
    """Generar reporte de pruebas"""
    print_section("REPORTE FINAL DE PRUEBAS")
    
    print("ğŸ“‹ RESUMEN EJECUTIVO:")
    print("-" * 30)
    
    # Analizar disponibilidad
    available_providers = []
    for provider, available in all_results.get('availability', {}).items():
        if available:
            available_providers.append(provider)
    
    print(f"âœ… Proveedores disponibles: {len(available_providers)}/2")
    print(f"   Activos: {', '.join(available_providers)}")
    
    # Analizar modelos
    total_models = sum(len(models) for models in all_results.get('models', {}).values())
    print(f"ğŸ¤– Total modelos disponibles: {total_models}")
    
    # Analizar pruebas de generaciÃ³n
    simple_tests = all_results.get('simple_generation', {})
    rag_tests = all_results.get('rag_generation', {})
    comparison_tests = all_results.get('comparison', {})
    
    print(f"ğŸ§ª Pruebas de generaciÃ³n simple: {len(simple_tests)} completadas")
    print(f"ğŸ“š Pruebas con RAG: {len(rag_tests)} completadas")
    print(f"âš–ï¸  Pruebas de comparaciÃ³n: {'âœ…' if comparison_tests else 'âŒ'}")
    
    # Status general
    if available_providers and total_models > 0:
        print(f"\nğŸ‰ ESTADO GENERAL: âœ… SISTEMA FUNCIONAL")
        print("   El LLM Service estÃ¡ listo para integraciÃ³n")
    else:
        print(f"\nâš ï¸  ESTADO GENERAL: ğŸ”§ REQUIERE CONFIGURACIÃ“N")
        print("   Verificar instalaciÃ³n de Ollama y/o API key de OpenAI")

def main():
    """FunciÃ³n principal de pruebas"""
    print("ğŸš€ INICIANDO SUITE DE PRUEBAS LLM SERVICE")
    print("=" * 60)
    
    results = {}
    
    # Test 1: InicializaciÃ³n
    service = test_service_initialization()
    if not service:
        print("âŒ No se puede continuar sin servicio inicializado")
        return
    
    # Test 2: Disponibilidad
    availability = test_provider_availability(service)
    results['availability'] = availability
    
    # Test 3: Modelos
    models = test_available_models(service)
    results['models'] = models
    
    # Test 4: GeneraciÃ³n simple
    simple_results = test_simple_generation(service, availability)
    results['simple_generation'] = simple_results
    
    # Test 5: RAG
    rag_results = test_rag_generation(service, availability)
    results['rag_generation'] = rag_results
    
    # Test 6: ComparaciÃ³n (si ambos disponibles)
    comparison_results = test_model_comparison(service, availability)
    results['comparison'] = comparison_results
    
    # Test 7: EstadÃ­sticas
    stats = test_service_stats(service)
    results['stats'] = stats
    
    # Test 8: Errores
    test_error_handling(service)
    
    # Reporte final
    generate_test_report(results)
    
    print(f"\nğŸ PRUEBAS COMPLETADAS")
    print("=" * 60)

if __name__ == "__main__":
    main()