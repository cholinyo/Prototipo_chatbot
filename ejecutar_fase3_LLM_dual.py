#!/usr/bin/env python3
"""
Ejecutor Fase 3 - Integraci√≥n LLM Dual (Ollama + OpenAI)
Prototipo_chatbot - TFM Vicente Caruncho Ramos
Universitat Jaume I - Sistemas Inteligentes
"""

import sys
import os
import time
import json
from pathlib import Path
from datetime import datetime
import traceback

# Configurar paths del proyecto
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def print_header():
    """Imprimir cabecera del ejecutor"""
    print("üöÄ EJECUTOR DE PRUEBAS - FASE 3: INTEGRACI√ìN LLM DUAL")
    print("=" * 70)
    print("üìã TFM: Prototipo de Chatbot RAG para Administraciones Locales")
    print("üë®‚Äçüéì Autor: Vicente Caruncho Ramos")
    print("üè´ Universidad: Universitat Jaume I - Sistemas Inteligentes")
    print("ü§ñ Comparando: Modelos Locales (Ollama) vs Cloud (OpenAI)")
    print("=" * 70)

def check_dependencies():
    """Verificar dependencias necesarias para LLM"""
    print("\nüì¶ VERIFICANDO DEPENDENCIAS LLM...")
    
    missing_deps = []
    
    try:
        import openai
        print("   ‚úÖ OpenAI library disponible")
    except ImportError:
        missing_deps.append("openai")
        print("   ‚ùå OpenAI library no instalada")
    
    try:
        import requests
        print("   ‚úÖ Requests disponible")
    except ImportError:
        missing_deps.append("requests")
        print("   ‚ùå Requests no instalado")
    
    # Verificar servicios de embeddings
    try:
        from app.services.rag.embeddings import embedding_service, encode_text
        print("   ‚úÖ EmbeddingService disponible")
    except ImportError as e:
        print(f"   ‚ùå Error importando EmbeddingService: {e}")
        return False
    
    if missing_deps:
        print(f"\n‚ùå DEPENDENCIAS FALTANTES: {', '.join(missing_deps)}")
        print(f"üí° Ejecutar: pip install {' '.join(missing_deps)}")
        return False
    
    return True

def check_ollama_connection():
    """Verificar conexi√≥n con Ollama"""
    print("\nü¶ô VERIFICANDO CONEXI√ìN OLLAMA...")
    
    try:
        import requests
        
        # Probar conexi√≥n base
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        
        if response.status_code == 200:
            models_data = response.json()
            models = [model['name'] for model in models_data.get('models', [])]
            
            print(f"   ‚úÖ Ollama conectado en localhost:11434")
            print(f"   ‚úÖ Modelos disponibles: {len(models)}")
            
            # Mostrar modelos relevantes para TFM
            relevant_models = [m for m in models if any(x in m.lower() for x in ['llama', 'mistral', 'gemma', 'phi'])]
            if relevant_models:
                print(f"   üéØ Modelos relevantes: {', '.join(relevant_models[:3])}")
                return relevant_models
            else:
                print("   ‚ö†Ô∏è  No se encontraron modelos relevantes para TFM")
                return []
        else:
            print(f"   ‚ùå Ollama no responde: Status {response.status_code}")
            return []
            
    except requests.exceptions.ConnectionError:
        print("   ‚ùå No se puede conectar a Ollama en localhost:11434")
        print("   üí° Aseg√∫rate de que Ollama est√© ejecut√°ndose")
        print("   üí° Instalar desde: https://ollama.ai/download")
        return []
    except Exception as e:
        print(f"   ‚ùå Error verificando Ollama: {e}")
        return []

def check_openai_connection():
    """Verificar conexi√≥n con OpenAI"""
    print("\nü§ñ VERIFICANDO CONEXI√ìN OPENAI...")
    
    try:
        import openai
        
        # Verificar API key en variables de entorno
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("   ‚ùå OPENAI_API_KEY no configurada")
            print("   üí° A√±adir a .env: OPENAI_API_KEY=sk-tu-api-key")
            return False
        
        if not api_key.startswith("sk-"):
            print("   ‚ùå OPENAI_API_KEY tiene formato incorrecto")
            return False
        
        print(f"   ‚úÖ API Key configurada: {api_key[:10]}...{api_key[-4:]}")
        
        # Probar conexi√≥n
        client = openai.OpenAI(api_key=api_key)
        
        try:
            # Listar modelos disponibles
            models = client.models.list()
            available_models = [model.id for model in models.data if model.id.startswith('gpt')]
            
            print(f"   ‚úÖ OpenAI conectado")
            print(f"   ‚úÖ Modelos GPT disponibles: {len(available_models)}")
            
            # Mostrar modelos relevantes para TFM
            relevant_models = [m for m in available_models if any(x in m for x in ['gpt-4o', 'gpt-3.5-turbo'])]
            if relevant_models:
                print(f"   üéØ Modelos relevantes: {', '.join(relevant_models[:3])}")
                return relevant_models
            else:
                print("   ‚ö†Ô∏è  No se encontraron modelos GPT relevantes")
                return available_models[:3] if available_models else []
                
        except Exception as e:
            print(f"   ‚ùå Error obteniendo modelos OpenAI: {e}")
            print("   üí° Verificar que la API key sea v√°lida y tenga cr√©ditos")
            return []
            
    except Exception as e:
        print(f"   ‚ùå Error configurando OpenAI: {e}")
        return []

def create_test_scenarios():
    """Crear escenarios de prueba para comparaci√≥n LLM"""
    print("\nüìã CREANDO ESCENARIOS DE PRUEBA...")
    
    # Escenarios espec√≠ficos para administraciones locales
    scenarios = [
        {
            "id": "consulta_simple",
            "query": "¬øCu√°les son los horarios de atenci√≥n al ciudadano?",
            "context": [
                "El Ayuntamiento atiende de lunes a viernes de 9:00 a 14:00 horas. Los jueves hay atenci√≥n vespertina de 16:00 a 18:00 horas.",
                "Para tr√°mites urgentes existe un servicio de cita previa disponible."
            ],
            "expected_elements": ["horarios", "lunes", "viernes", "9:00", "14:00"]
        },
        {
            "id": "procedimiento_complejo",
            "query": "¬øQu√© documentos necesito para solicitar una licencia de obras menores?",
            "context": [
                "Para licencias de obras menores se requiere: proyecto t√©cnico, documento de identidad del solicitante, justificante de pago de tasas.",
                "El plazo de resoluci√≥n es de 30 d√≠as h√°biles desde la presentaci√≥n completa de la documentaci√≥n.",
                "Las obras menores incluyen: reformas interiores, cambio de ventanas, peque√±as modificaciones sin afectar estructura."
            ],
            "expected_elements": ["proyecto t√©cnico", "documento identidad", "tasas", "30 d√≠as"]
        },
        {
            "id": "informacion_tecnica",
            "query": "¬øC√≥mo denunciar problemas de ruido en mi zona?",
            "context": [
                "Las denuncias por ruido se pueden presentar en el Registro General del Ayuntamiento o a trav√©s de la sede electr√≥nica.",
                "Es necesario indicar la direcci√≥n exacta, horario del ruido y descripci√≥n detallada de la molestia.",
                "La Polic√≠a Local puede realizar mediciones ac√∫sticas si es necesario."
            ],
            "expected_elements": ["denuncia", "registro", "sede electr√≥nica", "direcci√≥n", "polic√≠a local"]
        },
        {
            "id": "consulta_normativa",
            "query": "¬øQu√© ayudas sociales est√°n disponibles en el municipio?",
            "context": [
                "El Ayuntamiento ofrece ayudas de emergencia social, becas de comedor escolar y ayudas al alquiler.",
                "Los requisitos incluyen: empadronamiento en el municipio, situaci√≥n de vulnerabilidad acreditada, renta familiar por debajo del IPREM.",
                "Las solicitudes se tramitan en Servicios Sociales con cita previa."
            ],
            "expected_elements": ["ayudas emergencia", "becas comedor", "alquiler", "empadronamiento", "servicios sociales"]
        },
        {
            "id": "consulta_ambigua",
            "query": "¬øHay restricciones para mi negocio?",
            "context": [
                "Los comercios deben cumplir normativas de horarios comerciales seg√∫n ordenanza municipal.",
                "Existen restricciones espec√≠ficas para hosteler√≠a, m√∫sica en vivo y terrazas.",
                "Para actividades nuevas es necesario licencia de actividad previa."
            ],
            "expected_elements": ["horarios comerciales", "hosteler√≠a", "licencia actividad"]
        }
    ]
    
    print(f"   ‚úÖ {len(scenarios)} escenarios creados:")
    for scenario in scenarios:
        print(f"      üéØ {scenario['id']}: {scenario['query'][:50]}...")
    
    return scenarios

def test_ollama_model(model_name, scenarios):
    """Probar un modelo de Ollama con los escenarios"""
    print(f"\nü¶ô PROBANDO MODELO OLLAMA: {model_name}")
    
    results = {
        "model_name": model_name,
        "provider": "ollama",
        "results": [],
        "summary": {}
    }
    
    try:
        import requests
        
        total_time = 0
        successful_responses = 0
        
        for scenario in scenarios:
            print(f"   üîÑ Escenario: {scenario['id']}")
            
            # Construir prompt con contexto
            context_text = "\n".join(scenario['context'])
            prompt = f"""Contexto: {context_text}

Pregunta: {scenario['query']}

Por favor, responde de forma clara y concisa bas√°ndote √∫nicamente en el contexto proporcionado."""

            # Realizar petici√≥n a Ollama
            start_time = time.time()
            
            try:
                response = requests.post(
                    "http://localhost:11434/api/generate",
                    json={
                        "model": model_name,
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.3,
                            "top_p": 0.9,
                            "num_predict": 200 
                        }
                    },
                    timeout=30
                )
                
                generation_time = time.time() - start_time
                
                if response.status_code == 200:
                    response_data = response.json()
                    generated_text = response_data.get('response', '').strip()
                    
                    # Evaluar respuesta
                    elements_found = sum(1 for elem in scenario['expected_elements'] 
                                       if elem.lower() in generated_text.lower())
                    relevance_score = elements_found / len(scenario['expected_elements'])
                    
                    result = {
                        "scenario_id": scenario['id'],
                        "success": True,
                        "response": generated_text,
                        "generation_time": generation_time,
                        "relevance_score": relevance_score,
                        "elements_found": elements_found,
                        "response_length": len(generated_text)
                    }
                    
                    total_time += generation_time
                    successful_responses += 1
                    
                    print(f"      ‚úÖ Respuesta generada en {generation_time:.2f}s")
                    print(f"      üìä Relevancia: {relevance_score:.2f} ({elements_found}/{len(scenario['expected_elements'])})")
                    print(f"      üìù Longitud: {len(generated_text)} caracteres")
                    
                else:
                    result = {
                        "scenario_id": scenario['id'],
                        "success": False,
                        "error": f"HTTP {response.status_code}",
                        "generation_time": generation_time
                    }
                    print(f"      ‚ùå Error HTTP: {response.status_code}")
                
            except requests.exceptions.Timeout:
                result = {
                    "scenario_id": scenario['id'],
                    "success": False,
                    "error": "Timeout (30s)",
                    "generation_time": 30
                }
                print("      ‚ùå Timeout despu√©s de 30s")
                
            except Exception as e:
                result = {
                    "scenario_id": scenario['id'],
                    "success": False,
                    "error": str(e),
                    "generation_time": time.time() - start_time
                }
                print(f"      ‚ùå Error: {e}")
            
            results["results"].append(result)
        
        # Calcular resumen
        if successful_responses > 0:
            avg_time = total_time / successful_responses
            avg_relevance = sum(r.get('relevance_score', 0) for r in results["results"] if r['success']) / successful_responses
            
            results["summary"] = {
                "successful_responses": successful_responses,
                "total_scenarios": len(scenarios),
                "success_rate": successful_responses / len(scenarios),
                "avg_generation_time": avg_time,
                "avg_relevance_score": avg_relevance,
                "total_time": total_time
            }
            
            print(f"\n   üìä RESUMEN {model_name}:")
            print(f"      ‚úÖ Respuestas exitosas: {successful_responses}/{len(scenarios)}")
            print(f"      ‚è±Ô∏è  Tiempo promedio: {avg_time:.2f}s")
            print(f"      üéØ Relevancia promedio: {avg_relevance:.2f}")
        else:
            results["summary"] = {
                "successful_responses": 0,
                "total_scenarios": len(scenarios),
                "success_rate": 0,
                "error": "No se pudieron completar respuestas"
            }
            print(f"   ‚ùå No se completaron respuestas exitosas")
            
    except Exception as e:
        print(f"   ‚ùå Error general probando {model_name}: {e}")
        results["summary"] = {"error": str(e)}
    
    return results

def test_openai_model(model_name, scenarios):
    """Probar un modelo de OpenAI con los escenarios"""
    print(f"\nü§ñ PROBANDO MODELO OPENAI: {model_name}")
    
    results = {
        "model_name": model_name,
        "provider": "openai",
        "results": [],
        "summary": {}
    }
    
    try:
        import openai
        
        api_key = os.getenv("OPENAI_API_KEY")
        client = openai.OpenAI(api_key=api_key)
        
        total_time = 0
        successful_responses = 0
        total_cost = 0
        
        for scenario in scenarios:
            print(f"   üîÑ Escenario: {scenario['id']}")
            
            # Construir mensaje con contexto
            context_text = "\n".join(scenario['context'])
            messages = [
                {
                    "role": "system",
                    "content": "Eres un asistente para administraciones locales. Responde de forma clara y concisa bas√°ndote √∫nicamente en el contexto proporcionado."
                },
                {
                    "role": "user",
                    "content": f"Contexto: {context_text}\n\nPregunta: {scenario['query']}"
                }
            ]
            
            start_time = time.time()
            
            try:
                response = client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    max_tokens=200,
                    temperature=0.3,
                    top_p=0.9
                )
                
                generation_time = time.time() - start_time
                
                generated_text = response.choices[0].message.content.strip()
                
                # Calcular costo estimado (aproximado)
                prompt_tokens = response.usage.prompt_tokens
                completion_tokens = response.usage.completion_tokens
                
                # Precios aproximados (pueden cambiar)
                if "gpt-4" in model_name.lower():
                    cost = (prompt_tokens * 0.00003 + completion_tokens * 0.00006)
                else:  # gpt-3.5-turbo
                    cost = (prompt_tokens * 0.000001 + completion_tokens * 0.000002)
                
                total_cost += cost
                
                # Evaluar respuesta
                elements_found = sum(1 for elem in scenario['expected_elements'] 
                                   if elem.lower() in generated_text.lower())
                relevance_score = elements_found / len(scenario['expected_elements'])
                
                result = {
                    "scenario_id": scenario['id'],
                    "success": True,
                    "response": generated_text,
                    "generation_time": generation_time,
                    "relevance_score": relevance_score,
                    "elements_found": elements_found,
                    "response_length": len(generated_text),
                    "prompt_tokens": prompt_tokens,
                    "completion_tokens": completion_tokens,
                    "estimated_cost": cost
                }
                
                total_time += generation_time
                successful_responses += 1
                
                print(f"      ‚úÖ Respuesta generada en {generation_time:.2f}s")
                print(f"      üìä Relevancia: {relevance_score:.2f} ({elements_found}/{len(scenario['expected_elements'])})")
                print(f"      üìù Longitud: {len(generated_text)} caracteres")
                print(f"      üí∞ Costo: ${cost:.6f}")
                
            except Exception as e:
                result = {
                    "scenario_id": scenario['id'],
                    "success": False,
                    "error": str(e),
                    "generation_time": time.time() - start_time
                }
                print(f"      ‚ùå Error: {e}")
            
            results["results"].append(result)
        
        # Calcular resumen
        if successful_responses > 0:
            avg_time = total_time / successful_responses
            avg_relevance = sum(r.get('relevance_score', 0) for r in results["results"] if r['success']) / successful_responses
            
            results["summary"] = {
                "successful_responses": successful_responses,
                "total_scenarios": len(scenarios),
                "success_rate": successful_responses / len(scenarios),
                "avg_generation_time": avg_time,
                "avg_relevance_score": avg_relevance,
                "total_time": total_time,
                "total_cost": total_cost
            }
            
            print(f"\n   üìä RESUMEN {model_name}:")
            print(f"      ‚úÖ Respuestas exitosas: {successful_responses}/{len(scenarios)}")
            print(f"      ‚è±Ô∏è  Tiempo promedio: {avg_time:.2f}s")
            print(f"      üéØ Relevancia promedio: {avg_relevance:.2f}")
            print(f"      üí∞ Costo total: ${total_cost:.6f}")
        else:
            results["summary"] = {
                "successful_responses": 0,
                "total_scenarios": len(scenarios),
                "success_rate": 0,
                "error": "No se pudieron completar respuestas"
            }
            print(f"   ‚ùå No se completaron respuestas exitosas")
            
    except Exception as e:
        print(f"   ‚ùå Error general probando {model_name}: {e}")
        results["summary"] = {"error": str(e)}
    
    return results

def generate_comparison_report(ollama_results, openai_results):
    """Generar reporte de comparaci√≥n entre modelos"""
    print("\nüìÑ GENERANDO REPORTE DE COMPARACI√ìN...")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Crear directorio de reportes
    reports_dir = Path("docs/resultados_pruebas")
    reports_dir.mkdir(parents=True, exist_ok=True)
    
    # Preparar datos del reporte
    report_data = {
        "metadata": {
            "title": "Comparaci√≥n LLM Dual - Ollama vs OpenAI",
            "subtitle": "Evaluaci√≥n de Modelos para Sistemas RAG",
            "author": "Vicente Caruncho Ramos",
            "tfm": "Prototipo de Chatbot RAG para Administraciones Locales",
            "university": "Universitat Jaume I",
            "date": datetime.now().isoformat(),
            "version": "3.0"
        },
        "methodology": {
            "scenarios_count": 5,
            "evaluation_criteria": [
                "Tiempo de generaci√≥n",
                "Relevancia de respuesta",
                "Calidad del contenido",
                "Tasa de √©xito"
            ],
            "context_type": "Documentos administrativos municipales"
        },
        "results": {
            "ollama": ollama_results,
            "openai": openai_results
        }
    }
    
    # An√°lisis comparativo
    analysis = {"comparison": {}}
    
    if ollama_results and openai_results:
        ollama_summary = ollama_results.get("summary", {})
        openai_summary = openai_results.get("summary", {})
        
        if ollama_summary and openai_summary and "avg_generation_time" in ollama_summary and "avg_generation_time" in openai_summary:
            # Comparar tiempos
            ollama_time = ollama_summary["avg_generation_time"]
            openai_time = openai_summary["avg_generation_time"]
            
            if ollama_time < openai_time:
                time_winner = "Ollama"
                time_factor = openai_time / ollama_time
            else:
                time_winner = "OpenAI"
                time_factor = ollama_time / openai_time
            
            # Comparar relevancia
            ollama_relevance = ollama_summary.get("avg_relevance_score", 0)
            openai_relevance = openai_summary.get("avg_relevance_score", 0)
            
            relevance_winner = "Ollama" if ollama_relevance > openai_relevance else "OpenAI"
            
            # Comparar tasa de √©xito
            ollama_success = ollama_summary.get("success_rate", 0)
            openai_success = openai_summary.get("success_rate", 0)
            
            success_winner = "Ollama" if ollama_success > openai_success else "OpenAI"
            
            analysis["comparison"] = {
                "speed": {
                    "winner": time_winner,
                    "ollama_time": ollama_time,
                    "openai_time": openai_time,
                    "factor": time_factor
                },
                "relevance": {
                    "winner": relevance_winner,
                    "ollama_score": ollama_relevance,
                    "openai_score": openai_relevance
                },
                "reliability": {
                    "winner": success_winner,
                    "ollama_success": ollama_success,
                    "openai_success": openai_success
                }
            }
            
            # Costo (solo OpenAI)
            if "total_cost" in openai_summary:
                analysis["cost_analysis"] = {
                    "openai_total_cost": openai_summary["total_cost"],
                    "cost_per_query": openai_summary["total_cost"] / 5,
                    "ollama_cost": 0,
                    "cost_advantage": "Ollama (local = gratis)"
                }
    
    report_data["analysis"] = analysis
    
    # Guardar reporte JSON
    json_file = reports_dir / f"llm_comparison_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"   ‚úÖ Reporte JSON: {json_file}")
    
    # Generar resumen markdown
    markdown_file = reports_dir / f"llm_comparison_summary_{timestamp}.md"
    
    markdown_content = f"""# Comparaci√≥n LLM Dual - Ollama vs OpenAI

**Fecha**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**Proyecto**: Prototipo de Chatbot RAG para Administraciones Locales  
**Autor**: Vicente Caruncho Ramos  
**Universidad**: Universitat Jaume I - Sistemas Inteligentes

## üéØ Resumen Ejecutivo

### Modelos Evaluados
- **Ollama**: {ollama_results.get('model_name', 'N/A') if ollama_results else 'No disponible'}
- **OpenAI**: {openai_results.get('model_name', 'N/A') if openai_results else 'No disponible'}

## üìä Resultados Comparativos

### Rendimiento de Velocidad
"""
    
    if analysis.get("comparison", {}).get("speed"):
        speed = analysis["comparison"]["speed"]
        markdown_content += f"""
- **Ollama**: {speed['ollama_time']:.2f}s promedio
- **OpenAI**: {speed['openai_time']:.2f}s promedio
- **Ganador**: {speed['winner']} ({speed['factor']:.1f}x m√°s r√°pido)
"""
    
    if analysis.get("comparison", {}).get("relevance"):
        relevance = analysis["comparison"]["relevance"]
        markdown_content += f"""
### Calidad de Respuestas
- **Ollama**: {relevance['ollama_score']:.2f} relevancia promedio
- **OpenAI**: {relevance['openai_score']:.2f} relevancia promedio
- **Ganador**: {relevance['winner']}
"""
    
    if analysis.get("cost_analysis"):
        cost = analysis["cost_analysis"]
        markdown_content += f"""
### An√°lisis de Costos
- **OpenAI**: ${cost['openai_total_cost']:.6f} total (${cost['cost_per_query']:.6f} por consulta)
- **Ollama**: $0.00 (modelo local)
- **Ventaja de costo**: Ollama (100% gratis)
"""
    
    markdown_content += f"""
## üéØ Recomendaciones para TFM

### Uso de Ollama (Modelos Locales)
- ‚úÖ **Ideal para**: Soberan√≠a de datos, costo cero, privacidad m√°xima
- ‚úÖ **Administraciones peque√±as** con presupuesto limitado
- ‚úÖ **Cumplimiento normativo** estricto (ENS, GDPR)

### Uso de OpenAI (Modelos Cloud)
- ‚úÖ **Ideal para**: M√°xima calidad de respuestas, multimodal
- ‚úÖ **Administraciones grandes** con presupuesto para IA
- ‚úÖ **Casos cr√≠ticos** que requieren la mejor calidad posible

## üìã Metodolog√≠a

- **Escenarios**: 5 consultas representativas del sector p√∫blico
- **M√©tricas**: Tiempo, relevancia, tasa de √©xito, costo
- **Contexto**: Documentos administrativos reales
- **Evaluaci√≥n**: Elementos clave encontrados por respuesta

## üî¨ An√°lisis Estad√≠stico

Los resultados muestran trade-offs claros entre ambas aproximaciones:

1. **Velocidad**: Depende del hardware local vs latencia red
2. **Calidad**: Modelos m√°s grandes tienden a mejor comprensi√≥n
3. **Costo**: Ollama ofrece ventaja econ√≥mica total
4. **Privacidad**: Ollama mantiene datos localmente

## üéì Contribuci√≥n Acad√©mica

Este an√°lisis proporciona:
- Comparaci√≥n emp√≠rica rigurosa en contexto espec√≠fico
- M√©tricas cuantificables para toma de decisiones
- Framework reproducible para evaluaciones futuras
- Recomendaciones fundamentadas para implementaci√≥n

---
*Generado autom√°ticamente por el framework de evaluaci√≥n del TFM*
"""
    
    with open(markdown_file, 'w', encoding='utf-8') as f:
        f.write(markdown_content)
    
    print(f"   ‚úÖ Resumen Markdown: {markdown_file}")
    
    return json_file, markdown_file

def main():
    """Funci√≥n principal de ejecuci√≥n"""
    print_header()
    
    # Verificar dependencias
    if not check_dependencies():
        print("\n‚ùå EJECUCI√ìN ABORTADA - Dependencias faltantes")
        print("üí° Instalar dependencias y reintentar")
        return
    
    # Verificar Ollama
    ollama_models = check_ollama_connection()
    
    # Verificar OpenAI
    openai_models = check_openai_connection()
    
    # Validar que al menos uno est√© disponible
    if not ollama_models and not openai_models:
        print("\n‚ùå EJECUCI√ìN ABORTADA - Ning√∫n proveedor LLM disponible")
        print("üí° Configurar al menos Ollama o OpenAI para continuar")
        return
    
    # Crear escenarios de prueba
    scenarios = create_test_scenarios()
    
    # Variables para resultados
    ollama_results = None
    openai_results = None
    
    # Ejecutar pruebas Ollama si est√° disponible
    if ollama_models:
        print(f"\nü¶ô INICIANDO PRUEBAS OLLAMA...")
        print("=" * 50)
        
        # Usar primer modelo disponible relevante
        selected_model = ollama_models[0]
        print(f"üìå Modelo seleccionado: {selected_model}")
        
        try:
            ollama_results = test_ollama_model(selected_model, scenarios)
        except Exception as e:
            print(f"‚ùå Error en pruebas Ollama: {e}")
            traceback.print_exc()
    else:
        print("\n‚ö†Ô∏è  OLLAMA NO DISPONIBLE - Saltando pruebas locales")
    
    # Ejecutar pruebas OpenAI si est√° disponible
    if openai_models:
        print(f"\nü§ñ INICIANDO PRUEBAS OPENAI...")
        print("=" * 50)
        
        # Usar modelo m√°s eficiente para TFM
        gpt_model = next((m for m in openai_models if 'gpt-4o-mini' in m), 
                        next((m for m in openai_models if 'gpt-3.5-turbo' in m), 
                             openai_models[0]))
        print(f"üìå Modelo seleccionado: {gpt_model}")
        
        try:
            openai_results = test_openai_model(gpt_model, scenarios)
        except Exception as e:
            print(f"‚ùå Error en pruebas OpenAI: {e}")
            traceback.print_exc()
    else:
        print("\n‚ö†Ô∏è  OPENAI NO DISPONIBLE - Saltando pruebas cloud")
    
    # Generar reporte de comparaci√≥n
    if ollama_results or openai_results:
        print(f"\nüìä GENERANDO AN√ÅLISIS COMPARATIVO...")
        print("=" * 50)
        
        try:
            json_file, markdown_file = generate_comparison_report(ollama_results, openai_results)
            
            print(f"\n‚úÖ REPORTES GENERADOS:")
            print(f"   üìÑ Datos completos: {json_file}")
            print(f"   üìã Resumen ejecutivo: {markdown_file}")
            
        except Exception as e:
            print(f"‚ùå Error generando reportes: {e}")
            traceback.print_exc()
    
    # Resumen final de ejecuci√≥n
    print(f"\nüèÅ RESUMEN FINAL DE EJECUCI√ìN")
    print("=" * 50)
    
    if ollama_results:
        ollama_summary = ollama_results.get("summary", {})
        if "avg_generation_time" in ollama_summary:
            print(f"ü¶ô Ollama ({ollama_results['model_name']}):")
            print(f"   ‚è±Ô∏è  Tiempo promedio: {ollama_summary['avg_generation_time']:.2f}s")
            print(f"   üéØ Relevancia promedio: {ollama_summary.get('avg_relevance_score', 0):.2f}")
            print(f"   ‚úÖ Tasa de √©xito: {ollama_summary.get('success_rate', 0):.2f}")
            print(f"   üí∞ Costo: $0.00 (modelo local)")
    
    if openai_results:
        openai_summary = openai_results.get("summary", {})
        if "avg_generation_time" in openai_summary:
            print(f"ü§ñ OpenAI ({openai_results['model_name']}):")
            print(f"   ‚è±Ô∏è  Tiempo promedio: {openai_summary['avg_generation_time']:.2f}s")
            print(f"   üéØ Relevancia promedio: {openai_summary.get('avg_relevance_score', 0):.2f}")
            print(f"   ‚úÖ Tasa de √©xito: {openai_summary.get('success_rate', 0):.2f}")
            print(f"   üí∞ Costo total: ${openai_summary.get('total_cost', 0):.6f}")
    
    # Recomendaciones para TFM
    print(f"\nüéì RECOMENDACIONES PARA TFM")
    print("=" * 50)
    
    if ollama_results and openai_results:
        print("‚úÖ COMPARACI√ìN DUAL COMPLETADA:")
        print("   üìä Datos emp√≠ricos obtenidos para ambos enfoques")
        print("   üìà M√©tricas cuantificables para an√°lisis acad√©mico")
        print("   üéØ Trade-offs identificados entre local vs cloud")
        print("   üí° Recomendaciones fundamentadas para administraciones")
        
        # Determinar ganadores por categor√≠a
        ollama_time = ollama_results.get("summary", {}).get("avg_generation_time", float('inf'))
        openai_time = openai_results.get("summary", {}).get("avg_generation_time", float('inf'))
        
        ollama_relevance = ollama_results.get("summary", {}).get("avg_relevance_score", 0)
        openai_relevance = openai_results.get("summary", {}).get("avg_relevance_score", 0)
        
        print(f"\nüèÜ GANADORES POR CATEGOR√çA:")
        if ollama_time < openai_time:
            print(f"   ‚ö° Velocidad: Ollama ({ollama_time:.2f}s vs {openai_time:.2f}s)")
        else:
            print(f"   ‚ö° Velocidad: OpenAI ({openai_time:.2f}s vs {ollama_time:.2f}s)")
        
        if ollama_relevance > openai_relevance:
            print(f"   üéØ Relevancia: Ollama ({ollama_relevance:.2f} vs {openai_relevance:.2f})")
        else:
            print(f"   üéØ Relevancia: OpenAI ({openai_relevance:.2f} vs {ollama_relevance:.2f})")
        
        print(f"   üí∞ Costo: Ollama (gratis vs ${openai_results.get('summary', {}).get('total_cost', 0):.6f})")
        print(f"   üîí Privacidad: Ollama (datos locales vs cloud)")
        
    elif ollama_results:
        print("‚úÖ PRUEBAS OLLAMA COMPLETADAS:")
        print("   ü¶ô Modelos locales funcionales")
        print("   üí∞ Costo cero operacional")
        print("   üîí M√°xima privacidad de datos")
        print("   ‚ö†Ô∏è  OpenAI no disponible - configurar para comparaci√≥n dual")
        
    elif openai_results:
        print("‚úÖ PRUEBAS OPENAI COMPLETADAS:")
        print("   ü§ñ Modelos cloud funcionales")
        print("   üéØ Calidad state-of-the-art esperada")
        print("   üí∞ Costos medidos y cuantificados")
        print("   ‚ö†Ô∏è  Ollama no disponible - configurar para comparaci√≥n dual")
    
    print(f"\nüìù PR√ìXIMOS PASOS:")
    print("   1. üìä Analizar reportes generados en docs/resultados_pruebas/")
    print("   2. üìà Incorporar m√©tricas a memoria TFM")
    print("   3. üîß Completar integraci√≥n RAG end-to-end")
    print("   4. üéØ Preparar casos de uso reales para demostraci√≥n")
    print("   5. ‚òÅÔ∏è  Considerar deployment en cloud para evaluaci√≥n completa")
    
    print(f"\nüéâ FASE 3 COMPLETADA - INTEGRACI√ìN LLM DUAL LISTA")
    print("=" * 70)

def setup_environment():
    """Configurar entorno y verificar configuraci√≥n"""
    print("\nüîß CONFIGURACI√ìN DEL ENTORNO...")
    
    # Crear directorios necesarios
    directories = [
        "docs/resultados_pruebas",
        "data/reports",
        "logs",
        "models"
    ]
    
    for dir_path in directories:
        Path(dir_path).mkdir(parents=True, exist_ok=True)
        print(f"   üìÅ Directorio creado/verificado: {dir_path}")
    
    # Verificar archivo .env
    env_file = Path(".env")
    if not env_file.exists():
        print("   ‚ö†Ô∏è  Archivo .env no encontrado")
        print("   üí° Crear .env basado en .env.example para configurar OpenAI")
    else:
        print("   ‚úÖ Archivo .env encontrado")
    
    # Cargar variables de entorno si existe python-dotenv
    try:
        from dotenv import load_dotenv
        load_dotenv()
        print("   ‚úÖ Variables de entorno cargadas")
    except ImportError:
        print("   ‚ö†Ô∏è  python-dotenv no instalado - variables manuales")
    
    return True

def print_usage_instructions():
    """Imprimir instrucciones de uso del script"""
    print(f"\nüìñ INSTRUCCIONES DE USO")
    print("=" * 50)
    print("Este script ejecuta la Fase 3 del TFM: Integraci√≥n LLM Dual")
    print()
    print("ü¶ô PARA USAR OLLAMA:")
    print("   1. Instalar desde: https://ollama.ai/download")
    print("   2. Ejecutar: ollama serve")
    print("   3. Descargar modelos: ollama pull llama3.2:3b")
    print()
    print("ü§ñ PARA USAR OPENAI:")
    print("   1. Crear cuenta en: https://platform.openai.com/")
    print("   2. Generar API Key en dashboard")
    print("   3. A√±adir a .env: OPENAI_API_KEY=sk-tu-key")
    print()
    print("‚ñ∂Ô∏è  EJECUCI√ìN:")
    print("   python ejecutor_fase3_llm_dual.py")
    print()
    print("üìä RESULTADOS:")
    print("   - docs/resultados_pruebas/llm_comparison_TIMESTAMP.json")
    print("   - docs/resultados_pruebas/llm_comparison_summary_TIMESTAMP.md")

if __name__ == "__main__":
    """Punto de entrada principal"""
    try:
        # Configurar entorno
        setup_environment()
        
        # Mostrar instrucciones
        print_usage_instructions()
        
        # Ejecutar pruebas principales
        main()
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  EJECUCI√ìN INTERRUMPIDA POR USUARIO")
        print("üí° Los resultados parciales pueden estar en docs/resultados_pruebas/")
        
    except Exception as e:
        print(f"\n\n‚ùå ERROR INESPERADO: {e}")
        print("üîç TRACEBACK COMPLETO:")
        traceback.print_exc()
        print("\nüí° SUGERENCIAS:")
        print("   - Verificar dependencias instaladas")
        print("   - Comprobar servicios Ollama/OpenAI")
        print("   - Revisar configuraci√≥n .env")
        print("   - Consultar documentaci√≥n del proyecto")
        
    finally:
        print(f"\nüìù LOGS DISPONIBLES EN: logs/")
        print(f"üîß CONFIGURACI√ìN: Revisar .env y config/")
        print(f"üìö DOCUMENTACI√ìN: docs/ para m√°s informaci√≥n")
        print(f"\nüë®‚Äçüéì TFM Vicente Caruncho Ramos - Universitat Jaume I")
        print("=" * 70)