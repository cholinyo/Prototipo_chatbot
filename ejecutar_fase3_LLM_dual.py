#!/usr/bin/env python3
"""
Ejecutor Fase 3 - IntegraciÃ³n LLM Dual (Ollama + OpenAI)
Prototipo_chatbot - TFM Vicente Caruncho Ramos
Universitat Jaume I - Sistemas Inteligentes
"""

import sys
import os
import time
import json
from pathlib import Path
from datetime import datetime
import traceback

# Configurar paths del proyecto
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def print_header():
    """Imprimir cabecera del ejecutor"""
    print("ğŸš€ EJECUTOR DE PRUEBAS - FASE 3: INTEGRACIÃ“N LLM DUAL")
    print("=" * 70)
    print("ğŸ“‹ TFM: Prototipo de Chatbot RAG para Administraciones Locales")
    print("ğŸ‘¨â€ğŸ“ Autor: Vicente Caruncho Ramos")
    print("ğŸ« Universidad: Universitat Jaume I - Sistemas Inteligentes")
    print("ğŸ¤– Comparando: Modelos Locales (Ollama) vs Cloud (OpenAI)")
    print("=" * 70)

def check_dependencies():
    """Verificar dependencias necesarias para LLM"""
    print("\nğŸ“¦ VERIFICANDO DEPENDENCIAS LLM...")
    
    missing_deps = []
    
    try:
        import openai
        print("   âœ… OpenAI library disponible")
    except ImportError:
        missing_deps.append("openai")
        print("   âŒ OpenAI library no instalada")
    
    try:
        import requests
        print("   âœ… Requests disponible")
    except ImportError:
        missing_deps.append("requests")
        print("   âŒ Requests no instalado")
    
    # Verificar servicios de embeddings
    try:
        from app.services.rag.embeddings import embedding_service, encode_text
        print("   âœ… EmbeddingService disponible")
    except ImportError as e:
        print(f"   âŒ Error importando EmbeddingService: {e}")
        return False
    
    if missing_deps:
        print(f"\nâŒ DEPENDENCIAS FALTANTES: {', '.join(missing_deps)}")
        print(f"ğŸ’¡ Ejecutar: pip install {' '.join(missing_deps)}")
        return False
    
    return True

def check_ollama_connection():
    """Verificar conexiÃ³n con Ollama"""
    print("\nğŸ¦™ VERIFICANDO CONEXIÃ“N OLLAMA...")
    
    try:
        import requests
        
        # Probar conexiÃ³n base
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        
        if response.status_code == 200:
            models_data = response.json()
            models = [model['name'] for model in models_data.get('models', [])]
            
            print(f"   âœ… Ollama conectado en localhost:11434")
            print(f"   âœ… Modelos disponibles: {len(models)}")
            
            # Mostrar modelos relevantes para TFM
            relevant_models = [m for m in models if any(x in m.lower() for x in ['llama', 'mistral', 'gemma', 'phi'])]
            if relevant_models:
                print(f"   ğŸ¯ Modelos relevantes: {', '.join(relevant_models[:3])}")
                return relevant_models
            else:
                print("   âš ï¸  No se encontraron modelos relevantes para TFM")
                return []
        else:
            print(f"   âŒ Ollama no responde: Status {response.status_code}")
            return []
            
    except requests.exceptions.ConnectionError:
        print("   âŒ No se puede conectar a Ollama en localhost:11434")
        print("   ğŸ’¡ AsegÃºrate de que Ollama estÃ© ejecutÃ¡ndose")
        print("   ğŸ’¡ Instalar desde: https://ollama.ai/download")
        return []
    except Exception as e:
        print(f"   âŒ Error verificando Ollama: {e}")
        return []

def check_openai_connection():
    """Verificar conexiÃ³n con OpenAI"""
    print("\nğŸ¤– VERIFICANDO CONEXIÃ“N OPENAI...")
    
    try:
        import openai
        
        # Verificar API key en variables de entorno
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("   âŒ OPENAI_API_KEY no configurada")
            print("   ğŸ’¡ AÃ±adir a .env: OPENAI_API_KEY=sk-tu-api-key")
            return False
        
        if not api_key.startswith("sk-"):
            print("   âŒ OPENAI_API_KEY tiene formato incorrecto")
            return False
        
        print(f"   âœ… API Key configurada: {api_key[:10]}...{api_key[-4:]}")
        
        # Probar conexiÃ³n
        client = openai.OpenAI(api_key=api_key)
        
        try:
            # Listar modelos disponibles
            models = client.models.list()
            available_models = [model.id for model in models.data if model.id.startswith('gpt')]
            
            print(f"   âœ… OpenAI conectado")
            print(f"   âœ… Modelos GPT disponibles: {len(available_models)}")
            
            # Mostrar modelos relevantes para TFM
            relevant_models = [m for m in available_models if any(x in m for x in ['gpt-4o', 'gpt-3.5-turbo'])]
            if relevant_models:
                print(f"   ğŸ¯ Modelos relevantes: {', '.join(relevant_models[:3])}")
                return relevant_models
            else:
                print("   âš ï¸  No se encontraron modelos GPT relevantes")
                return available_models[:3] if available_models else []
                
        except Exception as e:
            print(f"   âŒ Error obteniendo modelos OpenAI: {e}")
            print("   ğŸ’¡ Verificar que la API key sea vÃ¡lida y tenga crÃ©ditos")
            return []
            
    except Exception as e:
        print(f"   âŒ Error configurando OpenAI: {e}")
        return []

def create_test_scenarios():
    """Crear escenarios de prueba para comparaciÃ³n LLM"""
    print("\nğŸ“‹ CREANDO ESCENARIOS DE PRUEBA...")
    
    # Escenarios especÃ­ficos para administraciones locales
    scenarios = [
        {
            "id": "consulta_simple",
            "query": "Â¿CuÃ¡les son los horarios de atenciÃ³n al ciudadano?",
            "context": [
                "El Ayuntamiento atiende de lunes a viernes de 9:00 a 14:00 horas. Los jueves hay atenciÃ³n vespertina de 16:00 a 18:00 horas.",
                "Para trÃ¡mites urgentes existe un servicio de cita previa disponible."
            ],
            "expected_elements": ["horarios", "lunes", "viernes", "9:00", "14:00"]
        },
        {
            "id": "procedimiento_complejo",
            "query": "Â¿QuÃ© documentos necesito para solicitar una licencia de obras menores?",
            "context": [
                "Para licencias de obras menores se requiere: proyecto tÃ©cnico, documento de identidad del solicitante, justificante de pago de tasas.",
                "El plazo de resoluciÃ³n es de 30 dÃ­as hÃ¡biles desde la presentaciÃ³n completa de la documentaciÃ³n.",
                "Las obras menores incluyen: reformas interiores, cambio de ventanas, pequeÃ±as modificaciones sin afectar estructura."
            ],
            "expected_elements": ["proyecto tÃ©cnico", "documento identidad", "tasas", "30 dÃ­as"]
        },
        {
            "id": "informacion_tecnica",
            "query": "Â¿CÃ³mo denunciar problemas de ruido en mi zona?",
            "context": [
                "Las denuncias por ruido se pueden presentar en el Registro General del Ayuntamiento o a travÃ©s de la sede electrÃ³nica.",
                "Es necesario indicar la direcciÃ³n exacta, horario del ruido y descripciÃ³n detallada de la molestia.",
                "La PolicÃ­a Local puede realizar mediciones acÃºsticas si es necesario."
            ],
            "expected_elements": ["denuncia", "registro", "sede electrÃ³nica", "direcciÃ³n", "policÃ­a local"]
        },
        {
            "id": "consulta_normativa",
            "query": "Â¿QuÃ© ayudas sociales estÃ¡n disponibles en el municipio?",
            "context": [
                "El Ayuntamiento ofrece ayudas de emergencia social, becas de comedor escolar y ayudas al alquiler.",
                "Los requisitos incluyen: empadronamiento en el municipio, situaciÃ³n de vulnerabilidad acreditada, renta familiar por debajo del IPREM.",
                "Las solicitudes se tramitan en Servicios Sociales con cita previa."
            ],
            "expected_elements": ["ayudas emergencia", "becas comedor", "alquiler", "empadronamiento", "servicios sociales"]
        },
        {
            "id": "consulta_ambigua",
            "query": "Â¿Hay restricciones para mi negocio?",
            "context": [
                "Los comercios deben cumplir normativas de horarios comerciales segÃºn ordenanza municipal.",
                "Existen restricciones especÃ­ficas para hostelerÃ­a, mÃºsica en vivo y terrazas.",
                "Para actividades nuevas es necesario licencia de actividad previa."
            ],
            "expected_elements": ["horarios comerciales", "hostelerÃ­a", "licencia actividad"]
        }
    ]
    
    print(f"   âœ… {len(scenarios)} escenarios creados:")
    for scenario in scenarios:
        print(f"      ğŸ¯ {scenario['id']}: {scenario['query'][:50]}...")
    
    return scenarios

def test_ollama_model(model_name, scenarios):
    """Probar un modelo de Ollama con los escenarios"""
    print(f"\nğŸ¦™ PROBANDO MODELO OLLAMA: {model_name}")
    
    results = {
        "model_name": model_name,
        "provider": "ollama",
        "results": [],
        "summary": {}
    }
    
    try:
        import requests
        
        total_time = 0
        successful_responses = 0
        
        for scenario in scenarios:
            print(f"   ğŸ”„ Escenario: {scenario['id']}")
            
            # Construir prompt con contexto
            context_text = "\n".join(scenario['context'])
            prompt = f"""Contexto: {context_text}

Pregunta: {scenario['query']}

Por favor, responde de forma clara y concisa basÃ¡ndote Ãºnicamente en el contexto proporcionado."""

            # Realizar peticiÃ³n a Ollama
            start_time = time.time()
            
            try:
                response = requests.post(
                    "http://localhost:11434/api/generate",
                    json={
                        "model": model_name,
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.3,
                            "top_p": 0.9,
                            "num_predict": 200 
                        }
                    },
                    timeout=30
                )
                
                generation_time = time.time() - start_time
                
                if response.status_code == 200:
                    response_data = response.json()
                    generated_text = response_data.get('response', '').strip()
                    
                    # Evaluar respuesta
                    elements_found = sum(1 for elem in scenario['expected_elements'] 
                                       if elem.lower() in generated_text.lower())
                    relevance_score = elements_found / len(scenario['expected_elements'])
                    
                    result = {
                        "scenario_id": scenario['id'],
                        "success": True,
                        "response": generated_text,
                        "generation_time": generation_time,
                        "relevance_score": relevance_score,
                        "elements_found": elements_found,
                        "response_length": len(generated_text)
                    }
                    
                    total_time += generation_time
                    successful_responses += 1
                    
                    print(f"      âœ… Respuesta generada en {generation_time:.2f}s")
                    print(f"      ğŸ“Š Relevancia: {relevance_score:.2f} ({elements_found}/{len(scenario['expected_elements'])})")
                    print(f"      ğŸ“ Longitud: {len(generated_text)} caracteres")
                    
                else:
                    result = {
                        "scenario_id": scenario['id'],
                        "success": False,
                        "error": f"HTTP {response.status_code}",
                        "generation_time": generation_time
                    }
                    print(f"      âŒ Error HTTP: {response.status_code}")
                
            except requests.exceptions.Timeout:
                result = {
                    "scenario_id": scenario['id'],
                    "success": False,
                    "error": "Timeout (30s)",
                    "generation_time": 30
                }
                print("      âŒ Timeout despuÃ©s de 30s")
                
            except Exception as e:
                result = {
                    "scenario_id": scenario['id'],
                    "success": False,
                    "error": str(e),
                    "generation_time": time.time() - start_time
                }
                print(f"      âŒ Error: {e}")
            
            results["results"].append(result)
        
        # Calcular resumen
        if successful_responses > 0:
            avg_time = total_time / successful_responses
            avg_relevance = sum(r.get('relevance_score', 0) for r in results["results"] if r['success']) / successful_responses
            
            results["summary"] = {
                "successful_responses": successful_responses,
                "total_scenarios": len(scenarios),
                "success_rate": successful_responses / len(scenarios),
                "avg_generation_time": avg_time,
                "avg_relevance_score": avg_relevance,
                "total_time": total_time
            }
            
            print(f"\n   ğŸ“Š RESUMEN {model_name}:")
            print(f"      âœ… Respuestas exitosas: {successful_responses}/{len(scenarios)}")
            print(f"      â±ï¸  Tiempo promedio: {avg_time:.2f}s")
            print(f"      ğŸ¯ Relevancia promedio: {avg_relevance:.2f}")
        else:
            results["summary"] = {
                "successful_responses": 0,
                "total_scenarios": len(scenarios),
                "success_rate": 0,
                "error": "No se pudieron completar respuestas"
            }
            print(f"   âŒ No se completaron respuestas exitosas")
            
    except Exception as e:
        print(f"   âŒ Error general probando {model_name}: {e}")
        results["summary"] = {"error": str(e)}
    
    return results

def test_openai_model(model_name, scenarios):
    """Probar un modelo de OpenAI con los escenarios"""
    print(f"\nğŸ¤– PROBANDO MODELO OPENAI: {model_name}")
    
    results = {
        "model_name": model_name,
        "provider": "openai",
        "results": [],
        "summary": {}
    }
    
    try:
        import openai
        
        api_key = os.getenv("OPENAI_API_KEY")
        client = openai.OpenAI(api_key=api_key)
        
        total_time = 0
        successful_responses = 0
        total_cost = 0
        
        for scenario in scenarios:
            print(f"   ğŸ”„ Escenario: {scenario['id']}")
            
            # Construir mensaje con contexto
            context_text = "\n".join(scenario['context'])
            messages = [
                {
                    "role": "system",
                    "content": "Eres un asistente para administraciones locales. Responde de forma clara y concisa basÃ¡ndote Ãºnicamente en el contexto proporcionado."
                },
                {
                    "role": "user",
                    "content": f"Contexto: {context_text}\n\nPregunta: {scenario['query']}"
                }
            ]
            
            start_time = time.time()
            
            try:
                response = client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    max_tokens=200,
                    temperature=0.3,
                    top_p=0.9
                )
                
                generation_time = time.time() - start_time
                
                generated_text = response.choices[0].message.content.strip()
                
                # Calcular costo estimado (aproximado)
                prompt_tokens = response.usage.prompt_tokens
                completion_tokens = response.usage.completion_tokens
                
                # Precios aproximados (pueden cambiar)
                if "gpt-4" in model_name.lower():
                    cost = (prompt_tokens * 0.00003 + completion_tokens * 0.00006)
                else:  # gpt-3.5-turbo
                    cost = (prompt_tokens * 0.000001 + completion_tokens * 0.000002)
                
                total_cost += cost
                
                # Evaluar respuesta
                elements_found = sum(1 for elem in scenario['expected_elements'] 
                                   if elem.lower() in generated_text.lower())
                relevance_score = elements_found / len(scenario['expected_elements'])
                
                result = {
                    "scenario_id": scenario['id'],
                    "success": True,
                    "response": generated_text,
                    "generation_time": generation_time,
                    "relevance_score": relevance_score,
                    "elements_found": elements_found,
                    "response_length": len(generated_text),
                    "prompt_tokens": prompt_tokens,
                    "completion_tokens": completion_tokens,
                    "estimated_cost": cost
                }
                
                total_time += generation_time
                successful_responses += 1
                
                print(f"      âœ… Respuesta generada en {generation_time:.2f}s")
                print(f"      ğŸ“Š Relevancia: {relevance_score:.2f} ({elements_found}/{len(scenario['expected_elements'])})")
                print(f"      ğŸ“ Longitud: {len(generated_text)} caracteres")
                print(f"      ğŸ’° Costo: ${cost:.6f}")
                
            except Exception as e:
                result = {
                    "scenario_id": scenario['id'],
                    "success": False,
                    "error": str(e),
                    "generation_time": time.time() - start_time
                }
                print(f"      âŒ Error: {e}")
            
            results["results"].append(result)
        
        # Calcular resumen
        if successful_responses > 0:
            avg_time = total_time / successful_responses
            avg_relevance = sum(r.get('relevance_score', 0) for r in results["results"] if r['success']) / successful_responses
            
            results["summary"] = {
                "successful_responses": successful_responses,
                "total_scenarios": len(scenarios),
                "success_rate": successful_responses / len(scenarios),
                "avg_generation_time": avg_time,
                "avg_relevance_score": avg_relevance,
                "total_time": total_time,
                "total_cost": total_cost
            }
            
            print(f"\n   ğŸ“Š RESUMEN {model_name}:")
            print(f"      âœ… Respuestas exitosas: {successful_responses}/{len(scenarios)}")
            print(f"      â±ï¸  Tiempo promedio: {avg_time:.2f}s")
            print(f"      ğŸ¯ Relevancia promedio: {avg_relevance:.2f}")
            print(f"      ğŸ’° Costo total: ${total_cost:.6f}")
        else:
            results["summary"] = {
                "successful_responses": 0,
                "total_scenarios": len(scenarios),
                "success_rate": 0,
                "error": "No se pudieron completar respuestas"
            }
            print(f"   âŒ No se completaron respuestas exitosas")
            
    except Exception as e:
        print(f"   âŒ Error general probando {model_name}: {e}")
        results["summary"] = {"error": str(e)}
    
    return results

def generate_comparison_report(ollama_results, openai_results):
    """Generar reporte de comparaciÃ³n entre modelos"""
    print("\nğŸ“„ GENERANDO REPORTE DE COMPARACIÃ“N...")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Crear directorio de reportes
    reports_dir = Path("docs/resultados_pruebas")
    reports_dir.mkdir(parents=True, exist_ok=True)
    
    # Preparar datos del reporte
    report_data = {
        "metadata": {
            "title": "ComparaciÃ³n LLM Dual - Ollama vs OpenAI",
            "subtitle": "EvaluaciÃ³n de Modelos para Sistemas RAG",
            "author": "Vicente Caruncho Ramos",
            "tfm": "Prototipo de Chatbot RAG para Administraciones Locales",
            "university": "Universitat Jaume I",
            "date": datetime.now().isoformat(),
            "version": "3.0"
        },
        "methodology": {
            "scenarios_count": 5,
            "evaluation_criteria": [
                "Tiempo de generaciÃ³n",
                "Relevancia de respuesta",
                "Calidad del contenido",
                "Tasa de Ã©xito"
            ],
            "context_type": "Documentos administrativos municipales"
        },
        "results": {
            "ollama": ollama_results,
            "openai": openai_results
        }
    }
    
    # AnÃ¡lisis comparativo
    analysis = {"comparison": {}}
    
    if ollama_results and openai_results:
        ollama_summary = ollama_results.get("summary", {})
        openai_summary = openai_results.get("summary", {})
        
        if ollama_summary and openai_summary and "avg_generation_time" in ollama_summary and "avg_generation_time" in openai_summary:
            # Comparar tiempos
            ollama_time = ollama_summary["avg_generation_time"]
            openai_time = openai_summary["avg_generation_time"]
            
            if ollama_time < openai_time:
                time_winner = "Ollama"
                time_factor = openai_time / ollama_time
            else:
                time_winner = "OpenAI"
                time_factor = ollama_time / openai_time
            
            # Comparar relevancia
            ollama_relevance = ollama_summary.get("avg_relevance_score", 0)
            openai_relevance = openai_summary.get("avg_relevance_score", 0)
            
            relevance_winner = "Ollama" if ollama_relevance > openai_relevance else "OpenAI"
            
            # Comparar tasa de Ã©xito
            ollama_success = ollama_summary.get("success_rate", 0)
            openai_success = openai_summary.get("success_rate", 0)
            
            success_winner = "Ollama" if ollama_success > openai_success else "OpenAI"
            
            analysis["comparison"] = {
                "speed": {
                    "winner": time_winner,
                    "ollama_time": ollama_time,
                    "openai_time": openai_time,
                    "factor": time_factor
                },
                "relevance": {
                    "winner": relevance_winner,
                    "ollama_score": ollama_relevance,
                    "openai_score": openai_relevance
                },
                "reliability": {
                    "winner": success_winner,
                    "ollama_success": ollama_success,
                    "openai_success": openai_success
                }
            }
            
            # Costo (solo OpenAI)
            if "total_cost" in openai_summary:
                analysis["cost_analysis"] = {
                    "openai_total_cost": openai_summary["total_cost"],
                    "cost_per_query": openai_summary["total_cost"] / 5,
                    "ollama_cost": 0,
                    "cost_advantage": "Ollama (local = gratis)"
                }
    
    report_data["analysis"] = analysis
    
    # Guardar reporte JSON
    json_file = reports_dir / f"llm_comparison_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"   âœ… Reporte JSON: {json_file}")
    
    # Generar resumen markdown
    markdown_file = reports_dir / f"llm_comparison_summary_{timestamp}.md"
    
    markdown_content = f"""# ComparaciÃ³n LLM Dual - Ollama vs OpenAI

**Fecha**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**Proyecto**: Prototipo de Chatbot RAG para Administraciones Locales  
**Autor**: Vicente Caruncho Ramos  
**Universidad**: Universitat Jaume I - Sistemas Inteligentes

## ğŸ¯ Resumen Ejecutivo

### Modelos Evaluados
- **Ollama**: {ollama_results.get('model_name', 'N/A') if ollama_results else 'No disponible'}
- **OpenAI**: {openai_results.get('model_name', 'N/A') if openai_results else 'No disponible'}

## ğŸ“Š Resultados Comparativos

### Rendimiento de Velocidad
"""
    
    if analysis.get("comparison", {}).get("speed"):
        speed = analysis["comparison"]["speed"]
        markdown_content += f"""
- **Ollama**: {speed['ollama_time']:.2f}s promedio
- **OpenAI**: {speed['openai_time']:.2f}s promedio
- **Ganador**: {speed['winner']} ({speed['factor']:.1f}x mÃ¡s rÃ¡pido)
"""
    
    if analysis.get("comparison", {}).get("relevance"):
        relevance = analysis["comparison"]["relevance"]
        markdown_content += f"""
### Calidad de Respuestas
- **Ollama**: {relevance['ollama_score']:.2f} relevancia promedio
- **OpenAI**: {relevance['openai_score']:.2f} relevancia promedio
- **Ganador**: {relevance['winner']}
"""
    
    if analysis.get("cost_analysis"):
        cost = analysis["cost_analysis"]
        markdown_content += f"""
### AnÃ¡lisis de Costos
- **OpenAI**: ${cost['openai_total_cost']:.6f} total (${cost['cost_per_query']:.6f} por consulta)
- **Ollama**: $0.00 (modelo local)
- **Ventaja de costo**: Ollama (100% gratis)
"""
    
    markdown_content += f"""
## ğŸ¯ Recomendaciones para TFM

### Uso de Ollama (Modelos Locales)
- âœ… **Ideal para**: SoberanÃ­a de datos, costo cero, privacidad mÃ¡xima
- âœ… **Administraciones pequeÃ±as** con presupuesto limitado
- âœ… **Cumplimiento normativo** estricto (ENS, GDPR)

### Uso de OpenAI (Modelos Cloud)
- âœ… **Ideal para**: MÃ¡xima calidad de respuestas, multimodal
- âœ… **Administraciones grandes** con presupuesto para IA
- âœ… **Casos crÃ­ticos** que requieren la mejor calidad posible

## ğŸ“‹ MetodologÃ­a

- **Escenarios**: 5 consultas representativas del sector pÃºblico
- **MÃ©tricas**: Tiempo, relevancia, tasa de Ã©xito, costo
- **Contexto**: Documentos administrativos reales
- **EvaluaciÃ³n**: Elementos clave encontrados por respuesta

## ğŸ”¬ AnÃ¡lisis EstadÃ­stico

Los resultados muestran trade-offs claros entre ambas aproximaciones:

1. **Velocidad**: Depende del hardware local vs latencia red
2. **Calidad**: Modelos mÃ¡s grandes tienden a mejor comprensiÃ³n
3. **Costo**: Ollama ofrece ventaja econÃ³mica total
4. **Privacidad**: Ollama mantiene datos localmente

## ğŸ“ ContribuciÃ³n AcadÃ©mica

Este anÃ¡lisis proporciona:
- ComparaciÃ³n empÃ­rica rigurosa en contexto especÃ­fico
- MÃ©tricas cuantificables para toma de decisiones
- Framework reproducible para evaluaciones futuras
- Recomendaciones fundamentadas para implementaciÃ³n

---
*Generado automÃ¡ticamente por el framework de evaluaciÃ³n del TFM*
"""
    
    with open(markdown_file, 'w', encoding='utf-8') as f:
        f.write(markdown_content)
    
    print(f"   âœ… Resumen Markdown: {markdown_file}")
    
    return json_file, markdown_file

def main():
    """FunciÃ³n principal de ejecuciÃ³n"""
    print_header()
    
    # Verificar dependencias
    if not check_dependencies():
        print("\nâŒ EJECUCIÃ“N ABORTADA - Dependencias faltantes")
        print("ğŸ’¡ Instalar dependencias y reintentar")
        return
    
    # Verificar Ollama
    ollama_models = check_ollama_connection()
    
    # Verificar OpenAI
    openai_models = check_openai_connection()
    
    # Validar que al menos uno estÃ© disponible
    if not ollama_models and not openai_models:
        print("\nâŒ EJECUCIÃ“N ABORTADA - NingÃºn proveedor LLM disponible")
        print("ğŸ’¡ Configurar al menos Ollama o OpenAI para continuar")
        return
    
    # Crear escenarios de prueba
    scenarios = create_test_scenarios()
    
    # Variables para resultados
    ollama_results = None
    openai_results = None
    
    # Ejecutar pruebas Ollama si estÃ¡ disponible
    if ollama_models:
        print(f"\nğŸ¦™ INICIANDO PRUEBAS OLLAMA...")
        print("=" * 50)
        
        # Usar primer modelo disponible relevante
        selected_model = ollama_models[0]
        print(f"ğŸ“Œ Modelo seleccionado: {selected_model}")
        
        try:
            ollama_results = test_ollama_model(selected_model, scenarios)
        except Exception as e:
            print(f"âŒ Error en pruebas Ollama: {e}")
            traceback.print_exc()
    else:
        print("\nâš ï¸  OLLAMA NO DISPONIBLE - Saltando pruebas locales")
    
    # Ejecutar pruebas OpenAI si estÃ¡ disponible
    if openai_models:
        print(f"\nğŸ¤– INICIANDO PRUEBAS OPENAI...")
        print("=" * 50)
        
        # Usar modelo mÃ¡s eficiente para TFM
        gpt_model = next((m for m in openai_models if 'gpt-4o-mini' in m), 
                        next((m for m in openai_models if 'gpt-3.5-turbo' in m), 
                             openai_models[0]))
        print(f"ğŸ“Œ Modelo seleccionado: {gpt_model}")
        
        try:
            openai_results = test_openai_model(gpt_model, scenarios)
        except Exception as e:
            print(f"âŒ Error en pruebas OpenAI: {e}")
            traceback.print_exc()
    else:
        print("\nâš ï¸  OPENAI NO DISPONIBLE - Saltando pruebas cloud")
    
    # Generar reporte de comparaciÃ³n
    if ollama_results or openai_results:
        print(f"\nğŸ“Š GENERANDO ANÃLISIS COMPARATIVO...")
        print("=" * 50)
        
        try:
            json_file, markdown_file = generate_comparison_report(ollama_results, openai_results)
            
            print(f"\nâœ… REPORTES GENERADOS:")
            print(f"   ğŸ“„ Datos completos: {json_file}")
            print(f"   ğŸ“‹ Resumen ejecutivo: {markdown_file}")
            
        except Exception as e:
            print(f"âŒ Error generando reportes: {e}")
            traceback.print_exc()
    
    # Resumen final de ejecuciÃ³n
    print(f"\nğŸ RESUMEN FINAL DE EJECUCIÃ“N")
    print("=" * 50)
    
    if ollama_results:
        ollama_summary = ollama_results.get("summary", {})
        if "avg_generation_time" in ollama_summary:
            print(f"ğŸ¦™ Ollama ({ollama_results['model_name']}):")
            print(f"   â±ï¸  Tiempo promedio: {ollama_summary['avg_generation_time']:.2f}s")
            print(f"   ğŸ¯ Relevancia promedio: {ollama_summary.get('avg_relevance_score', 0):.2f}")
            print(f"   âœ… Tasa de Ã©xito: {ollama_summary.get('success_rate', 0):.2f}")
            print(f"   ğŸ’° Costo: $0.00 (modelo local)")
    
    if openai_results:
        openai_summary = openai_results.get("summary", {})
        if "avg_generation_time" in openai_summary:
            print(f"ğŸ¤– OpenAI ({openai_results['model_name']}):")
            print(f"   â±ï¸  Tiempo promedio: {openai_summary['avg_generation_time']:.2f}s")
            print(f"   ğŸ¯ Relevancia promedio: {openai_summary.get('avg_relevance_score', 0):.2f}")
            print(f"   âœ… Tasa de Ã©xito: {openai_summary.get('success_rate', 0):.2f}")
            print(f"   ğŸ’° Costo total: ${openai_summary.get('total_cost', 0):.6f}")
    
    # Recomendaciones para TFM
    print(f"\nğŸ“ RECOMENDACIONES PARA TFM")
    print("=" * 50)
    
    if ollama_results and openai_results:
        print("âœ… COMPARACIÃ“N DUAL COMPLETADA:")
        print("   ğŸ“Š Datos empÃ­ricos obtenidos para ambos enfoques")
        print("   ğŸ“ˆ MÃ©tricas cuantificables para anÃ¡lisis acadÃ©mico")
        print("   ğŸ¯ Trade-offs identificados entre local vs cloud")
        print("   ğŸ’¡ Recomendaciones fundamentadas para administraciones")
        
        # Determinar ganadores por categorÃ­a
        ollama_time = ollama_results.get("summary", {}).get("avg_generation_time", float('inf'))
        openai_time = openai_results.get("summary", {}).get("avg_generation_time", float('inf'))
        
        ollama_relevance = ollama_results.get("summary", {}).get("avg_relevance_score", 0)
        openai_relevance = openai_results.get("summary", {}).get("avg_relevance_score", 0)
        
        print(f"\nğŸ† GANADORES POR CATEGORÃA:")
        if ollama_time < openai_time:
            print(f"   âš¡ Velocidad: Ollama ({ollama_time:.2f}s vs {openai_time:.2f}s)")
        else:
            print(f"   âš¡ Velocidad: OpenAI ({openai_time:.2f}s vs {ollama_time:.2f}s)")
        
        if ollama_relevance > openai_relevance:
            print(f"   ğŸ¯ Relevancia: Ollama ({ollama_relevance:.2f} vs {openai_relevance:.2f})")
        else:
            print(f"   ğŸ¯ Relevancia: OpenAI ({openai_relevance:.2f} vs {ollama_relevance:.2f})")
        
        print(f"   ğŸ’° Costo: Ollama (gratis vs ${openai_results.get('summary', {}).get('total_cost', 0):.6f})")
        print(f"   ğŸ”’ Privacidad: Ollama (datos locales vs cloud)")
        
    elif ollama_results:
        print("âœ… PRUEBAS OLLAMA COMPLETADAS:")
        print("   ğŸ¦™ Modelos locales funcionales")
        print("   ğŸ’° Costo cero operacional")
        print("   ğŸ”’ MÃ¡xima privacidad de datos")
        print("   âš ï¸  OpenAI no disponible - configurar para comparaciÃ³n dual")
        
    elif openai_results:
        print("âœ… PRUEBAS OPENAI COMPLETADAS:")
        print("   ğŸ¤– Modelos cloud funcionales")
        print("   ğŸ¯ Calidad state-of-the-art esperada")
        print("   ğŸ’° Costos medidos y cuantificados")
        print("   âš ï¸  Ollama no disponible - configurar para comparaciÃ³n dual")
    
    print(f"\nğŸ“ PRÃ“XIMOS PASOS:")
    print("   1. ğŸ“Š Analizar reportes generados en docs/resultados_pruebas/")
    print("   2. ğŸ“ˆ Incorporar mÃ©tricas a memoria TFM")
    print("   3. ğŸ”§ Completar integraciÃ³n RAG end-to-end")
    print("   4. ğŸ¯ Preparar casos de uso reales para demostraciÃ³n")
    print("   5. â˜ï¸  Considerar deployment en cloud para evaluaciÃ³n completa")
    
    print(f"\nğŸ‰ FASE 3 COMPLETADA - INTEGRACIÃ“N LLM DUAL LISTA")
    print("=" * 70)

def setup_environment():
    """Configurar entorno y verificar configuraciÃ³n"""
    print("\nğŸ”§ CONFIGURACIÃ“N DEL ENTORNO...")
    
    # Crear directorios necesarios
    directories = [
        "docs/resultados_pruebas",
        "data/reports",
        "logs",
        "models"
    ]
    
    for dir_path in directories:
        Path(dir_path).mkdir(parents=True, exist_ok=True)
        print(f"   ğŸ“ Directorio creado/verificado: {dir_path}")
    
    # Verificar archivo .env
    env_file = Path(".env")
    if not env_file.exists():
        print("   âš ï¸  Archivo .env no encontrado")
        print("   ğŸ’¡ Crear .env basado en .env.example para configurar OpenAI")
    else:
        print("   âœ… Archivo .env encontrado")
    
    # Cargar variables de entorno si existe python-dotenv
    try:
        from dotenv import load_dotenv
        load_dotenv()
        print("   âœ… Variables de entorno cargadas")
    except ImportError:
        print("   âš ï¸  python-dotenv no instalado - variables manuales")
    
    return True

def print_usage_instructions():
    """Imprimir instrucciones de uso del script"""
    print(f"\nğŸ“– INSTRUCCIONES DE USO")
    print("=" * 50)
    print("Este script ejecuta la Fase 3 del TFM: IntegraciÃ³n LLM Dual")
    print()
    print("ğŸ¦™ PARA USAR OLLAMA:")
    print("   1. Instalar desde: https://ollama.ai/download")
    print("   2. Ejecutar: ollama serve")
    print("   3. Descargar modelos: ollama pull llama3.2:3b")
    print()
    print("ğŸ¤– PARA USAR OPENAI:")
    print("   1. Crear cuenta en: https://platform.openai.com/")
    print("   2. Generar API Key en dashboard")
    print("   3. AÃ±adir a .env: OPENAI_API_KEY=sk-tu-key")
    print()
    print("â–¶ï¸  EJECUCIÃ“N:")
    print("   python ejecutor_fase3_llm_dual.py")
    print()
    print("ğŸ“Š RESULTADOS:")
    print("   - docs/resultados_pruebas/llm_comparison_TIMESTAMP.json")
    print("   - docs/resultados_pruebas/llm_comparison_summary_TIMESTAMP.md")

if __name__ == "__main__":
    """Punto de entrada principal"""
    try:
        # Configurar entorno
        setup_environment()
        
        # Mostrar instrucciones
        print_usage_instructions()
        
        # Ejecutar pruebas principales
        main()
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  EJECUCIÃ“N INTERRUMPIDA POR USUARIO")
        print("ğŸ’¡ Los resultados parciales pueden estar en docs/resultados_pruebas/")
        
    except Exception as e:
        print(f"\n\nâŒ ERROR INESPERADO: {e}")
        print("ğŸ” TRACEBACK COMPLETO:")
        traceback.print_exc()
        print("\nğŸ’¡ SUGERENCIAS:")
        print("   - Verificar dependencias instaladas")
        print("   - Comprobar servicios Ollama/OpenAI")
        print("   - Revisar configuraciÃ³n .env")
        print("   - Consultar documentaciÃ³n del proyecto")
        
    finally:
        print(f"\nğŸ“ LOGS DISPONIBLES EN: logs/")
        print(f"ğŸ”§ CONFIGURACIÃ“N: Revisar .env y config/")
        print(f"ğŸ“š DOCUMENTACIÃ“N: docs/ para mÃ¡s informaciÃ³n")
        print(f"\nğŸ‘¨â€ğŸ“ TFM Vicente Caruncho Ramos - Universitat Jaume I")
        print("=" * 70)