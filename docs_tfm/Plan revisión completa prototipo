# üìã Plan de Revisi√≥n Completa del Prototipo TFM
## Chatbot RAG para Administraciones Locales

> **Vicente Caruncho Ramos - TFM Sistemas Inteligentes**  
> **Universitat Jaume I - Curso 2024-2025**  
> **Fecha de revisi√≥n**: Agosto 2025

---

## üéØ Objetivos de la Revisi√≥n

### **Objetivo Principal**
Verificar funcionalmente todos los componentes del prototipo, documentar su rendimiento y generar evidencia emp√≠rica para la memoria del TFM.

### **Objetivos Espec√≠ficos**
1. **Validar funcionamiento** de cada componente arquitect√≥nico
2. **Generar m√©tricas cuantitativas** para comparaci√≥n acad√©mica
3. **Documentar decisiones t√©cnicas** con justificaci√≥n cient√≠fica
4. **Preparar material visual** (screenshots, gr√°ficos, tablas) para TFM
5. **Identificar optimizaciones** y √°reas de mejora

---

## üìä Metodolog√≠a de Revisi√≥n

### **Enfoque por Capas Arquitect√≥nicas**
```
Capa de Datos (Embeddings) 
    ‚Üì
Capa de Almacenamiento (Vector Stores)
    ‚Üì
Capa de Inteligencia (LLMs + RAG)
    ‚Üì
Capa de Aplicaci√≥n (Web Interface)
    ‚Üì
Capa de Evaluaci√≥n (Benchmarking)
```

### **Criterios de Evaluaci√≥n**
- ‚úÖ **Funcionalidad**: ¬øEl componente funciona seg√∫n especificaciones?
- üìä **Rendimiento**: ¬øQu√© m√©tricas cuantitativas obtenemos?
- üîß **Calidad t√©cnica**: ¬øEl c√≥digo es mantenible y escalable?
- üìù **Documentaci√≥n**: ¬øEst√° preparado para replicaci√≥n acad√©mica?

---

## üß™ Fase 1: Sistema de Embeddings
### **Duraci√≥n estimada**: 1-2 d√≠as

#### **Pruebas Funcionales**

##### **1.1 Verificaci√≥n de Inicializaci√≥n**
```python
# Prueba: Carga correcta del modelo
from app.services.rag.embeddings import embedding_service

# Test 1: Modelo cargado correctamente
assert embedding_service.model is not None
print(f"‚úÖ Modelo cargado: {embedding_service.model_name}")

# Test 2: Dimensiones correctas
test_text = "Ejemplo de texto para prueba"
embedding = embedding_service.encode(test_text)
assert embedding.shape[0] == 384  # all-MiniLM-L6-v2
print(f"‚úÖ Dimensi√≥n embeddings: {embedding.shape}")
```

##### **1.2 Performance de Cache LRU**
```python
# Prueba: Eficiencia del cache
import time

texts = [f"Texto de prueba {i}" for i in range(1000)]

# Primera ejecuci√≥n (sin cache)
start_time = time.time()
embeddings_1 = embedding_service.encode_batch(texts)
time_without_cache = time.time() - start_time

# Segunda ejecuci√≥n (con cache)
start_time = time.time()
embeddings_2 = embedding_service.encode_batch(texts)
time_with_cache = time.time() - start_time

cache_speedup = time_without_cache / time_with_cache
print(f"‚úÖ Speedup con cache: {cache_speedup:.2f}x")
print(f"‚úÖ Cache hit rate: {embedding_service.cache_hit_rate:.2%}")
```

##### **1.3 Batch Processing Efficiency**
```python
# Prueba: Comparar procesamiento individual vs batch
import numpy as np

texts = [f"Documento administrativo {i}" for i in range(100)]

# Procesamiento individual
start_time = time.time()
individual_embeddings = [embedding_service.encode(text) for text in texts]
individual_time = time.time() - start_time

# Procesamiento batch
start_time = time.time()
batch_embeddings = embedding_service.encode_batch(texts, batch_size=32)
batch_time = time.time() - start_time

batch_speedup = individual_time / batch_time
print(f"‚úÖ Speedup batch processing: {batch_speedup:.2f}x")
```

#### **M√©tricas a Documentar para TFM**
- **Tiempo de inicializaci√≥n** del modelo
- **Throughput** (textos/segundo) individual vs batch
- **Cache hit rate** bajo diferentes cargas
- **Uso de memoria** durante operaciones
- **Speedup factor** con optimizaciones

#### **Resultado Esperado**
- ‚úÖ Documento t√©cnico: `docs/evaluacion_embeddings.md`
- ‚úÖ Gr√°ficos de rendimiento para TFM
- ‚úÖ Justificaci√≥n t√©cnica de decisiones arquitect√≥nicas

---

## üóÑÔ∏è Fase 2: Vector Stores Comparison
### **Duraci√≥n estimada**: 2-3 d√≠as

#### **Pruebas Comparativas FAISS vs ChromaDB**

##### **2.1 Benchmark de Inserci√≥n**
```python
# Prueba: Velocidad de indexaci√≥n de documentos
from app.services.rag.faiss_store import FAISSStore
from app.services.rag.chromadb_store import ChromaDBStore
import time

# Preparar datos de prueba
chunks = load_test_documents(1000)  # 1000 documentos administrativos

# Test FAISS
faiss_store = FAISSStore()
start_time = time.time()
faiss_store.add_documents(chunks)
faiss_insertion_time = time.time() - start_time

# Test ChromaDB
chromadb_store = ChromaDBStore()
start_time = time.time()
chromadb_store.add_documents(chunks)
chromadb_insertion_time = time.time() - start_time

print(f"FAISS inserci√≥n: {faiss_insertion_time:.2f}s")
print(f"ChromaDB inserci√≥n: {chromadb_insertion_time:.2f}s")
print(f"FAISS es {chromadb_insertion_time/faiss_insertion_time:.2f}x m√°s r√°pido")
```

##### **2.2 Benchmark de B√∫squeda**
```python
# Prueba: Latencia y precisi√≥n de b√∫squedas
queries = [
    "¬øC√≥mo solicitar una licencia de obra?",
    "Procedimiento para empadronamiento",
    "Horarios de atenci√≥n al ciudadano",
    "Requisitos para licencia de actividad",
    "Tr√°mites online disponibles"
]

# Medir latencia
faiss_times = []
chromadb_times = []

for query in queries:
    query_embedding = embedding_service.encode(query)
    
    # FAISS
    start_time = time.time()
    faiss_results = faiss_store.search(query_embedding, k=5)
    faiss_times.append(time.time() - start_time)
    
    # ChromaDB
    start_time = time.time()
    chromadb_results = chromadb_store.search(query_embedding, k=5)
    chromadb_times.append(time.time() - start_time)

avg_faiss_time = np.mean(faiss_times)
avg_chromadb_time = np.mean(chromadb_times)

print(f"Latencia promedio FAISS: {avg_faiss_time*1000:.2f}ms")
print(f"Latencia promedio ChromaDB: {avg_chromadb_time*1000:.2f}ms")
```

##### **2.3 Uso de Memoria**
```python
# Prueba: Consumo de memoria y almacenamiento
import psutil
import os

# Medir memoria antes
memory_before = psutil.Process().memory_info().rss / 1024 / 1024  # MB

# Cargar vector stores con datos
faiss_store.load_index()
faiss_memory = psutil.Process().memory_info().rss / 1024 / 1024 - memory_before

chromadb_store.load_collection()
chromadb_memory = psutil.Process().memory_info().rss / 1024 / 1024 - faiss_memory

# Medir almacenamiento en disco
faiss_disk_size = get_directory_size("data/vectorstore/faiss")
chromadb_disk_size = get_directory_size("data/vectorstore/chromadb")

print(f"FAISS - RAM: {faiss_memory:.1f}MB, Disco: {faiss_disk_size:.1f}MB")
print(f"ChromaDB - RAM: {chromadb_memory:.1f}MB, Disco: {chromadb_disk_size:.1f}MB")
```

##### **2.4 Persistencia y Recuperaci√≥n**
```python
# Prueba: Robustez de persistencia
# Test 1: Guardar estado
faiss_store.save_index()
chromadb_store.persist()

# Test 2: Simular reinicio (limpiar memoria)
del faiss_store, chromadb_store

# Test 3: Cargar desde disco
faiss_store_recovered = FAISSStore()
faiss_store_recovered.load_index()
assert faiss_store_recovered.get_document_count() > 0

chromadb_store_recovered = ChromaDBStore()
chromadb_store_recovered.load_collection()
assert chromadb_store_recovered.get_document_count() > 0

print("‚úÖ Persistencia verificada para ambos stores")
```

#### **M√©tricas a Documentar para TFM**
- **Velocidad de inserci√≥n** (docs/segundo)
- **Latencia de b√∫squeda** (ms/query)
- **Throughput de consultas** (queries/segundo)
- **Uso de memoria RAM** (MB)
- **Almacenamiento en disco** (MB)
- **Tiempo de recuperaci√≥n** post-reinicio
- **Precision@K** para consultas test

#### **Resultado Esperado**
- ‚úÖ Tabla comparativa completa para TFM
- ‚úÖ Gr√°ficos de rendimiento por m√©trica
- ‚úÖ Matriz de decisi√≥n: cu√°ndo usar cada tecnolog√≠a
- ‚úÖ An√°lisis estad√≠stico de significancia

---

## ü§ñ Fase 3: Integraci√≥n LLM Dual
### **Duraci√≥n estimada**: 2-3 d√≠as

#### **Pruebas de Modelos Locales (Ollama)**

##### **3.1 Verificaci√≥n de Conectividad Ollama**
```python
# Prueba: Conexi√≥n con servidor Ollama local
from app.services.llm_service import LLMService

llm_service = LLMService()

# Test conexi√≥n
try:
    response = llm_service.test_ollama_connection()
    print("‚úÖ Ollama server conectado")
    
    # Listar modelos disponibles
    models = llm_service.list_ollama_models()
    print(f"‚úÖ Modelos locales disponibles: {models}")
    
except Exception as e:
    print(f"‚ùå Error conectando Ollama: {e}")
```

##### **3.2 Pruebas de Generaci√≥n Local**
```python
# Prueba: Generaci√≥n con modelos locales
test_query = "¬øCu√°les son los horarios de atenci√≥n al ciudadano?"
test_context = [
    DocumentChunk(
        content="El Ayuntamiento atiende de lunes a viernes de 9:00 a 14:00",
        metadata={"source": "web_municipal", "type": "horarios"}
    )
]

for model in ["llama3.2:3b", "mistral:7b", "gemma2:2b"]:
    try:
        start_time = time.time()
        response = llm_service.generate_local(
            query=test_query,
            context=test_context,
            model=model
        )
        generation_time = time.time() - start_time
        
        print(f"‚úÖ {model}: {generation_time:.2f}s")
        print(f"   Respuesta: {response.content[:100]}...")
        print(f"   Tokens: {response.tokens_used}")
        
    except Exception as e:
        print(f"‚ùå {model}: Error - {e}")
```

#### **Pruebas de Modelos Cloud (OpenAI)**

##### **3.3 Verificaci√≥n de API OpenAI**
```python
# Prueba: Conexi√≥n con API OpenAI
try:
    response = llm_service.test_openai_connection()
    print("‚úÖ OpenAI API conectada")
    
    # Test con diferentes modelos
    openai_models = ["gpt-4o", "gpt-4o-mini", "gpt-3.5-turbo"]
    
    for model in openai_models:
        try:
            start_time = time.time()
            response = llm_service.generate_openai(
                query=test_query,
                context=test_context,
                model=model
            )
            generation_time = time.time() - start_time
            
            print(f"‚úÖ {model}: {generation_time:.2f}s")
            print(f"   Respuesta: {response.content[:100]}...")
            print(f"   Costo estimado: ${response.estimated_cost:.4f}")
            
        except Exception as e:
            print(f"‚ùå {model}: Error - {e}")
            
except Exception as e:
    print(f"‚ùå Error OpenAI API: {e}")
```

##### **3.4 Comparaci√≥n Dual Autom√°tica**
```python
# Prueba: Sistema de comparaci√≥n autom√°tica
comparison_queries = [
    "¬øC√≥mo solicitar el certificado de empadronamiento?",
    "¬øQu√© documentos necesito para la licencia de obra menor?",
    "¬øCu√°l es el procedimiento para darse de alta como aut√≥nomo?",
    "¬øD√≥nde puedo consultar el estado de mi expediente?",
    "¬øQu√© ayudas sociales est√°n disponibles este a√±o?"
]

for query in comparison_queries:
    print(f"\nüîç Query: {query}")
    
    # Obtener contexto relevante del RAG
    context = rag_service.retrieve_context(query, k=3)
    
    # Generar respuestas con ambos tipos de modelo
    comparison = llm_service.generate_dual_comparison(
        query=query,
        context=context,
        local_model="llama3.2:3b",
        openai_model="gpt-4o-mini"
    )
    
    print(f"üè† Local ({comparison.local_time:.2f}s): {comparison.local_response[:150]}...")
    print(f"‚òÅÔ∏è Cloud ({comparison.openai_time:.2f}s): {comparison.openai_response[:150]}...")
    print(f"üí∞ Costo: ${comparison.openai_cost:.4f}")
    print(f"‚ö° Speedup: {comparison.openai_time/comparison.local_time:.2f}x")
```

#### **M√©tricas a Documentar para TFM**
- **Latencia de generaci√≥n** por modelo (segundos)
- **Throughput** (tokens/segundo)
- **Calidad de respuestas** (evaluaci√≥n manual/autom√°tica)
- **Costos operativos** por consulta
- **Disponibilidad** y robustez de conexi√≥n
- **Escalabilidad** bajo carga

#### **Resultado Esperado**
- ‚úÖ Matriz comparativa completa Local vs Cloud
- ‚úÖ An√°lisis de costos y escalabilidad
- ‚úÖ Recomendaciones de uso por escenario
- ‚úÖ Justificaci√≥n t√©cnica para administraciones p√∫blicas

---

## üîó Fase 4: Sistema RAG End-to-End
### **Duraci√≥n estimada**: 2 d√≠as

#### **Pruebas de Pipeline Completo**

##### **4.1 Ingesta Multimodal**
```python
# Prueba: Pipeline completo de ingesta
from app.services.data_ingestion import DataIngestionService

ingestion_service = DataIngestionService()

# Test 1: Documentos PDF
pdf_results = ingestion_service.ingest_pdf(
    "test_documents/reglamento_municipal.pdf"
)
print(f"‚úÖ PDF procesado: {len(pdf_results.chunks)} chunks")

# Test 2: Documentos DOCX
docx_results = ingestion_service.ingest_docx(
    "test_documents/procedimientos_administrativos.docx"
)
print(f"‚úÖ DOCX procesado: {len(docx_results.chunks)} chunks")

# Test 3: Web Scraping
web_results = ingestion_service.ingest_url(
    "https://ejemplo-ayuntamiento.es/tramites"
)
print(f"‚úÖ Web procesada: {len(web_results.chunks)} chunks")

# Test 4: API Data
api_results = ingestion_service.ingest_api(
    endpoint="https://api-municipal.es/servicios",
    auth_token="test_token"
)
print(f"‚úÖ API procesada: {len(api_results.chunks)} chunks")
```

##### **4.2 Pipeline RAG Completo**
```python
# Prueba: Flujo RAG end-to-end
from app.services.rag_service import RAGService

rag_service = RAGService()

test_scenarios = [
    {
        "query": "¬øC√≥mo solicitar una licencia de apertura?",
        "expected_sources": ["reglamento_municipal", "web_tramites"]
    },
    {
        "query": "¬øCu√°les son los impuestos municipales vigentes?",
        "expected_sources": ["ordenanza_fiscal", "web_municipal"]
    },
    {
        "query": "¬øD√≥nde est√° ubicado el registro civil?",
        "expected_sources": ["directorio_servicios", "web_contacto"]
    }
]

for scenario in test_scenarios:
    print(f"\nüîç Testing: {scenario['query']}")
    
    # 1. Retrieve (b√∫squeda sem√°ntica)
    start_time = time.time()
    retrieved_docs = rag_service.retrieve(scenario['query'], k=5)
    retrieve_time = time.time() - start_time
    
    # 2. Augment (preparar contexto)
    context = rag_service.prepare_context(retrieved_docs)
    
    # 3. Generate (respuesta con LLM)
    start_time = time.time()
    response = rag_service.generate(scenario['query'], context)
    generate_time = time.time() - start_time
    
    # Validar resultados
    found_sources = [doc.metadata['source'] for doc in retrieved_docs]
    expected_found = any(exp in found_sources for exp in scenario['expected_sources'])
    
    print(f"‚úÖ Retrieve: {retrieve_time*1000:.1f}ms, {len(retrieved_docs)} docs")
    print(f"‚úÖ Generate: {generate_time*1000:.1f}ms")
    print(f"‚úÖ Sources found: {expected_found}")
    print(f"‚úÖ Response: {response.content[:100]}...")
```

##### **4.3 Trazabilidad y Transparencia**
```python
# Prueba: Sistema de trazabilidad
query = "¬øQu√© documentos necesito para renovar el DNI?"

# Generar respuesta con trazabilidad completa
traced_response = rag_service.generate_with_traceability(query)

print("üîç Consulta:", traced_response.query)
print("üìä M√©tricas:")
print(f"  - Tiempo total: {traced_response.total_time:.3f}s")
print(f"  - Docs recuperados: {len(traced_response.sources)}")
print(f"  - Modelo usado: {traced_response.model_used}")
print(f"  - Tokens: {traced_response.tokens_used}")

print("\nüìö Fuentes utilizadas:")
for i, source in enumerate(traced_response.sources, 1):
    print(f"  {i}. {source.title}")
    print(f"     Relevancia: {source.relevance_score:.3f}")
    print(f"     Fragmento: {source.content[:100]}...")

print(f"\nüí¨ Respuesta: {traced_response.content}")
```

#### **M√©tricas a Documentar para TFM**
- **Tiempo de respuesta total** (retrieve + generate)
- **Precisi√≥n de recuperaci√≥n** (documentos relevantes encontrados)
- **Cobertura de fuentes** por tipo de consulta
- **Calidad de respuestas** con contexto RAG vs sin contexto
- **Trazabilidad** y transparencia del sistema

#### **Resultado Esperado**
- ‚úÖ Demostraci√≥n funcional del sistema completo
- ‚úÖ M√©tricas de rendimiento end-to-end
- ‚úÖ Validaci√≥n de casos de uso reales
- ‚úÖ Evidencia de transparencia y trazabilidad

---

## üåê Fase 5: Interface Web y UX
### **Duraci√≥n estimada**: 1 d√≠a

#### **Pruebas de Interfaz de Usuario**

##### **5.1 Dashboard Principal**
```python
# Prueba: M√©tricas del dashboard
# Verificar que todas las m√©tricas se muestran correctamente

# Test en navegador:
# 1. Abrir http://localhost:5000
# 2. Verificar m√©tricas del sistema
# 3. Comprobar gr√°ficos de rendimiento
# 4. Validar estado de servicios

dashboard_tests = [
    "‚úÖ M√©tricas de embeddings actualizadas",
    "‚úÖ Estado vector stores mostrado",
    "‚úÖ Estad√≠sticas de uso visibles",
    "‚úÖ Gr√°ficos de rendimiento funcionando",
    "‚úÖ Links de navegaci√≥n operativos"
]
```

##### **5.2 Chat Interface**
```python
# Prueba: Funcionalidad del chat
chat_tests = [
    {
        "input": "¬øCu√°les son los horarios de atenci√≥n?",
        "expected": "Debe mostrar horarios municipales",
        "sources_expected": True
    },
    {
        "input": "¬øC√≥mo solicitar certificado de empadronamiento?",
        "expected": "Procedimiento paso a paso",
        "sources_expected": True
    },
    {
        "input": "¬øQu√© tiempo hace hoy?",
        "expected": "Informaci√≥n no disponible en documentos",
        "sources_expected": False
    }
]

# Test manual en navegador:
# 1. Abrir http://localhost:5000/chat
# 2. Probar cada query de test
# 3. Verificar respuestas y fuentes citadas
# 4. Comprobar historial de conversaci√≥n
```

##### **5.3 Sistema de Comparaci√≥n**
```python
# Prueba: Comparaci√≥n lado a lado
# Test en navegador:
# 1. Abrir interfaz de comparaci√≥n
# 2. Ingresar query de prueba
# 3. Verificar respuestas de ambos modelos
# 4. Comprobar m√©tricas mostradas (tiempo, costo)
# 5. Validar interfaz responsive

comparison_tests = [
    "‚úÖ Respuesta local generada",
    "‚úÖ Respuesta cloud generada", 
    "‚úÖ M√©tricas de tiempo mostradas",
    "‚úÖ Costos calculados correctamente",
    "‚úÖ Fuentes citadas en ambas respuestas"
]
```

#### **Documentaci√≥n Visual para TFM**
- **Screenshots** de todas las interfaces
- **Flujos de usuario** documentados
- **An√°lisis de UX** para t√©cnicos municipales
- **M√©tricas de usabilidad** (tiempo de tarea, tasa de √©xito)

#### **Resultado Esperado**
- ‚úÖ Interface completamente funcional
- ‚úÖ Material visual para TFM (screenshots, videos)
- ‚úÖ Validaci√≥n de UX con casos reales
- ‚úÖ Documentaci√≥n de dise√±o centrado en usuario

---

## üìà Fase 6: Benchmarking Acad√©mico Final
### **Duraci√≥n estimada**: 2-3 d√≠as

#### **Ejecutar Framework Cient√≠fico Completo**

##### **6.1 Benchmark Reproducible**
```python
# Ejecutar script de benchmarking acad√©mico
from scripts.comparison_faiss_vs_chromadb import run_comprehensive_benchmark

# Configuraci√≥n de benchmark
benchmark_config = {
    "dataset_sizes": [100, 500, 1000, 2000],
    "query_sets": ["administrativos", "ciudadanos", "tecnicos"],
    "embedding_models": ["all-MiniLM-L6-v2"],
    "vector_stores": ["faiss", "chromadb"],
    "metrics": ["latency", "throughput", "memory", "accuracy"],
    "repetitions": 5,  # Para significancia estad√≠stica
    "output_format": ["csv", "json", "latex"]
}

# Ejecutar benchmark completo
results = run_comprehensive_benchmark(benchmark_config)

# Generar reportes autom√°ticos
generate_academic_report(results, output_path="reports/benchmark_tfm.pdf")
```

##### **6.2 An√°lisis Estad√≠stico**
```python
# An√°lisis de significancia estad√≠stica
from scipy import stats
import pandas as pd

# Cargar resultados del benchmark
results_df = pd.read_csv("data/reports/benchmark_results.csv")

# Test de hip√≥tesis: FAISS vs ChromaDB
faiss_latency = results_df[results_df['store'] == 'faiss']['latency_ms']
chromadb_latency = results_df[results_df['store'] == 'chromadb']['latency_ms']

# T-test para diferencias significativas
t_stat, p_value = stats.ttest_ind(faiss_latency, chromadb_latency)

print(f"T-statistic: {t_stat:.4f}")
print(f"P-value: {p_value:.6f}")
print(f"Significativo (p<0.05): {p_value < 0.05}")

# Calcular effect size (Cohen's d)
effect_size = (faiss_latency.mean() - chromadb_latency.mean()) / \
              np.sqrt(((faiss_latency.std()**2 + chromadb_latency.std()**2) / 2))
print(f"Effect size (Cohen's d): {effect_size:.4f}")
```

##### **6.3 Generaci√≥n de Material TFM**
```python
# Script para generar autom√°ticamente material del TFM
from utils.tfm_generator import TFMContentGenerator

generator = TFMContentGenerator()

# Generar tablas LaTeX
generator.generate_comparison_table(
    data=results_df,
    output="tables/tabla_comparativa_stores.tex"
)

# Generar gr√°ficos acad√©micos
generator.generate_performance_plots(
    data=results_df,
    output_dir="figures/",
    formats=["pdf", "png"]
)

# Generar an√°lisis textual
generator.generate_results_analysis(
    data=results_df,
    template="templates/analisis_resultados.md",
    output="docs/analisis_empirico_tfm.md"
)
```

#### **M√©tricas Finales para TFM**
- **An√°lisis comparativo completo** con significancia estad√≠stica
- **Gr√°ficos de rendimiento** en calidad de publicaci√≥n
- **Tablas de resultados** formateadas para LaTeX
- **An√°lisis de trade-offs** t√©cnicos y econ√≥micos
- **Recomendaciones** basadas en evidencia emp√≠rica

#### **Resultado Esperado**
- ‚úÖ Evidencia emp√≠rica robusta para TFM
- ‚úÖ Material gr√°fico y tabular acad√©mico
- ‚úÖ An√°lisis estad√≠stico riguroso
- ‚úÖ Conclusiones fundamentadas en datos

---

## üìã Checklist de Deliverables

### **Documentaci√≥n T√©cnica**
- [ ] `docs/evaluacion_embeddings.md` - An√°lisis del sistema de embeddings
- [ ] `docs/comparativa_vector_stores.md` - FAISS vs ChromaDB
- [ ] `docs/integracion_llm_dual.md` - Local vs Cloud
- [ ] `docs/evaluacion_rag_completo.md` - Sistema end-to-end
- [ ] `docs/analisis_empirico_tfm.md` - Resultados estad√≠sticos

### **Material Visual TFM**
- [ ] `figures/performance_embeddings.pdf` - Gr√°ficos de rendimiento
- [ ] `figures/comparison_vector_stores.pdf` - Comparativas visuales
- [ ] `figures/architecture_diagram.pdf` - Diagrama arquitect√≥nico
- [ ] `screenshots/interface_screenshots/` - Capturas de interfaz
- [ ] `tables/` - Tablas LaTeX para memoria

### **Resultados Emp√≠ricos**
- [ ] `data/reports/benchmark_results.csv` - Datos crudos
- [ ] `data/reports/statistical_analysis.json` - An√°lisis estad√≠stico
- [ ] `data/reports/tfm_summary_report.pdf` - Reporte ejecutivo

### **C√≥digo Verificado**
- [ ] Todos los tests unitarios pasando (‚úÖ 100%)
- [ ] Integraci√≥n LLM dual funcional
- [ ] Pipeline RAG end-to-end operativo
- [ ] Interface web completamente funcional
- [ ] Scripts de benchmarking ejecutados

---

## üóìÔ∏è Cronograma Estimado

| Fase | Duraci√≥n | Entregables | Estado |
|------|----------|-------------|---------|
| **Fase 1**: Embeddings | 1-2 d√≠as | An√°lisis t√©cnico + m√©tricas | ‚è≥ Pendiente |
| **Fase 2**: Vector Stores | 2-3 d√≠as | Comparativa emp√≠rica | ‚è≥ Pendiente |
| **Fase 3**: LLM Integration | 2-3 d√≠as | Evaluaci√≥n dual | ‚è≥ Pendiente |
| **Fase 4**: RAG End-to-End | 2 d√≠as | Validaci√≥n completa | ‚è≥ Pendiente |
| **Fase 5**: Interface Web | 1 d√≠a | Material visual TFM | ‚è≥ Pendiente |
| **