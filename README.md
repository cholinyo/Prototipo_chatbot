# ðŸ¤– Prototipo_chatbot

**Sistema de Chatbot RAG para Administraciones Locales**  
*Trabajo Final de MÃ¡ster - Vicente Caruncho Ramos*  
*Tutor: Rafael Berlanga Llavori*

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![Flask](https://img.shields.io/badge/Flask-2.3+-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)
![Status](https://img.shields.io/badge/Status-En%20Desarrollo-orange.svg)

## ðŸ“‹ DescripciÃ³n

Sistema conversacional basado en arquitectura RAG (Retrieval-Augmented Generation) diseÃ±ado especÃ­ficamente para administraciones locales espaÃ±olas. Permite a tÃ©cnicos municipales consultar informaciÃ³n sobre procedimientos administrativos, normativas y servicios pÃºblicos mediante lenguaje natural.

### âœ¨ CaracterÃ­sticas Principales

- **ðŸ” RAG Avanzado**: RecuperaciÃ³n semÃ¡ntica desde mÃºltiples fuentes (documentos, APIs, web, BBDD)
- **âš–ï¸ ComparaciÃ³n de Modelos**: EvaluaciÃ³n directa entre modelos locales (Ollama) y comerciales (OpenAI)
- **ðŸ›¡ï¸ Seguridad Local**: Cumplimiento ENS y CCN-TEC 014, procesamiento local de datos sensibles
- **ðŸ”§ Arquitectura Modular**: Componentes intercambiables y configuraciÃ³n flexible
- **ðŸ“Š MÃ©tricas Detalladas**: Monitoreo de rendimiento, tokens y tiempos de respuesta
- **ðŸ“¥ Ingesta Multimodal**: Procesamiento de PDF, DOCX, Excel, CSV y contenido web

## ðŸ—ï¸ Arquitectura del Sistema

```mermaid
graph TB
    A[Usuario] --> B[Interfaz Web]
    B --> C[Flask App]
    C --> D[API Routes]
    
    D --> E[LLM Service]
    E --> F[Ollama Local]
    E --> G[OpenAI API]
    
    D --> H[RAG Service]
    H --> I[Vector Store]
    I --> J[FAISS]
    I --> K[ChromaDB]
    
    H --> L[Embedding Service]
    L --> M[sentence-transformers]
    
    D --> N[Ingestion Service]
    N --> O[Document Processor]
    N --> P[Web Scraper]
    N --> Q[API Connector]
    
    O --> R[PDF/DOCX/TXT]
    P --> S[HTML/Web]
    Q --> T[REST APIs]
```

## ðŸš€ Inicio RÃ¡pido

### Prerrequisitos

- **Python 3.9+**
- **Git**
- **Ollama** (para modelos locales) - [Instalar Ollama](https://ollama.ai/)
- **OpenAI API Key** (opcional, para modelos OpenAI)

### 1. Clonar y Configurar

```bash
# Clonar repositorio
git clone https://github.com/tuusuario/prototipo_chatbot.git
cd prototipo_chatbot

# Crear entorno virtual
python -m venv venv

# Activar entorno virtual
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# Instalar dependencias
pip install --upgrade pip
pip install -r requirements.txt
```

### 2. Configurar Variables de Entorno

```bash
# Copiar archivo de ejemplo
cp .env.example .env

# Editar .env con tus configuraciones
# Especialmente OPENAI_API_KEY si planeas usar OpenAI
```

### 3. Configurar Ollama (Modelos Locales)

```bash
# Instalar modelos recomendados
ollama pull llama3.2:3b
ollama pull mistral:7b
ollama pull gemma2:2b

# Verificar modelos instalados
ollama list
```

### 4. Ejecutar la AplicaciÃ³n

```bash
# Ejecutar script de setup (primera vez)
python scripts/setup_project.py

# Iniciar aplicaciÃ³n
python run.py
```

### 5. Acceder a la AplicaciÃ³n

Abre tu navegador y ve a: **http://localhost:5000**

## ðŸ“ Estructura del Proyecto

```
Prototipo_chatbot/
â”œâ”€â”€ app/                          # AplicaciÃ³n principal
â”‚   â”œâ”€â”€ core/                     # ConfiguraciÃ³n y logging
â”‚   â”‚   â”œâ”€â”€ config.py            # GestiÃ³n de configuraciÃ³n
â”‚   â”‚   â””â”€â”€ logger.py            # Sistema de logging
â”‚   â”œâ”€â”€ models/                   # Modelos de datos
â”‚   â”‚   â””â”€â”€ __init__.py          # Entidades y validaciones
â”‚   â”œâ”€â”€ services/                 # Servicios de negocio
â”‚   â”‚   â”œâ”€â”€ rag/                 # Sistema RAG
â”‚   â”‚   â”œâ”€â”€ ingestion/           # Ingesta de datos
â”‚   â”‚   â””â”€â”€ llm_service.py       # Servicio de LLMs
â”‚   â”œâ”€â”€ routes/                   # Rutas web
â”‚   â”‚   â”œâ”€â”€ api.py               # API REST
â”‚   â”‚   â”œâ”€â”€ chat.py              # Chat web
â”‚   â”‚   â””â”€â”€ main.py              # Rutas principales
â”‚   â”œâ”€â”€ templates/                # Templates HTML
â”‚   â”‚   â”œâ”€â”€ base.html            # Template base
â”‚   â”‚   â”œâ”€â”€ index.html           # PÃ¡gina principal
â”‚   â”‚   â”œâ”€â”€ chat/                # Templates de chat
â”‚   â”‚   â””â”€â”€ errors/              # PÃ¡ginas de error
â”‚   â”œâ”€â”€ static/                   # Archivos estÃ¡ticos
â”‚   â”‚   â”œâ”€â”€ css/                 # Estilos CSS
â”‚   â”‚   â””â”€â”€ js/                  # JavaScript
â”‚   â””â”€â”€ __init__.py              # Factory de aplicaciÃ³n
â”œâ”€â”€ config/                       # Configuraciones
â”‚   â””â”€â”€ settings.yaml            # ConfiguraciÃ³n principal
â”œâ”€â”€ data/                         # Datos del sistema
â”‚   â”œâ”€â”€ documents/               # Documentos para ingesta
â”‚   â”œâ”€â”€ vectorstore/             # Almacenes vectoriales
â”‚   â””â”€â”€ temp/                    # Archivos temporales
â”œâ”€â”€ scripts/                      # Scripts de utilidad
â”‚   â””â”€â”€ setup_project.py         # Script de configuraciÃ³n
â”œâ”€â”€ tests/                        # Tests automatizados
â”œâ”€â”€ logs/                         # Archivos de log
â”œâ”€â”€ requirements.txt              # Dependencias Python
â”œâ”€â”€ run.py                        # Punto de entrada
â”œâ”€â”€ .env.example                  # Variables de entorno ejemplo
â””â”€â”€ README.md                     # Esta documentaciÃ³n
```

## ðŸ”§ ConfiguraciÃ³n

### ConfiguraciÃ³n Principal

El archivo `config/settings.yaml` contiene toda la configuraciÃ³n del sistema:

```yaml
# ConfiguraciÃ³n de modelos
models:
  local:
    default: "llama3.2:3b"
    endpoint: "http://localhost:11434"
  openai:
    default: "gpt-4o-mini"

# ConfiguraciÃ³n RAG
rag:
  enabled: true
  k_default: 5
  chunk_size: 500
  similarity_threshold: 0.7

# Vector stores
vector_stores:
  default: "faiss"
  faiss:
    path: "data/vectorstore/faiss"
```

### Variables de Entorno (.env)

```env
# Desarrollo
FLASK_ENV=development
FLASK_DEBUG=True

# OpenAI (opcional)
OPENAI_API_KEY=sk-tu-api-key-aqui

# Modelos por defecto
DEFAULT_LOCAL_MODEL=llama3.2:3b
DEFAULT_OPENAI_MODEL=gpt-4o-mini

# ConfiguraciÃ³n RAG
RAG_K_DEFAULT=5
CHUNK_SIZE=500
```

## ðŸ’¬ Uso del Sistema

### Interfaz de Chat

1. **Acceder al Chat**: Ir a `/chat` o hacer clic en "Chat RAG"
2. **Configurar Modelo**: Seleccionar proveedor (Ollama/OpenAI) y modelo especÃ­fico
3. **Habilitar RAG**: Activar recuperaciÃ³n de documentos para contexto
4. **Hacer Consultas**: Escribir preguntas sobre administraciÃ³n local

### Ejemplos de Consultas

```
Â¿QuÃ© documentos necesito para una licencia de obra?
Â¿CÃ³mo tramito una subvenciÃ³n municipal?
Â¿QuÃ© ordenanzas regulan el ruido en espacios pÃºblicos?
Â¿CuÃ¡les son los plazos para presentar alegaciones?
```

### Modo ComparaciÃ³n

Activa el modo comparaciÃ³n para evaluar respuestas de mÃºltiples modelos:

1. Hacer clic en el botÃ³n de comparaciÃ³n âš–ï¸
2. La misma consulta se enviarÃ¡ a todos los proveedores disponibles
3. Ver respuestas lado a lado con mÃ©tricas de rendimiento

## ðŸ“Š Dashboard y MÃ©tricas

### Dashboard Principal

Accede a `/dashboard` para ver:

- **MÃ©tricas de Uso**: Consultas totales, tiempo de respuesta promedio
- **Estado de Servicios**: RAG, LLM, ingesta
- **GrÃ¡ficos de Rendimiento**: Uso temporal, distribuciÃ³n de modelos
- **Actividad Reciente**: Log de acciones del sistema

### API de MÃ©tricas

```bash
# Estado del sistema
GET /api/status

# EstadÃ­sticas completas
GET /api/stats

# Modelos disponibles
GET /api/models
```

## ðŸ“¥ Ingesta de Datos

### Formatos Soportados

- **Documentos**: PDF, DOCX, TXT, RTF, MD
- **Hojas de CÃ¡lculo**: XLSX, XLS, CSV
- **Web**: HTML, scraping de URLs
- **APIs**: Endpoints REST

### Agregar Documentos

1. **VÃ­a Interfaz Web**: Usar la funciÃ³n de carga (en desarrollo)
2. **VÃ­a API**:

```bash
# Procesar documentos
POST /api/ingestion/documents
{
  "file_paths": ["ruta/documento.pdf"]
}

# Procesar URLs
POST /api/ingestion/urls
{
  "urls": ["https://ejemplo.com/pagina.html"]
}
```

3. **VÃ­a CÃ³digo**:

```python
from app.services.ingestion import process_documents

# Procesar archivos
chunks = process_documents([
    "data/documents/procedimientos.pdf",
    "data/documents/normativas.docx"
])

# AÃ±adir al sistema RAG
from app.services.rag import add_documents_to_rag
add_documents_to_rag(chunks)
```

## ðŸ” API REST

### Endpoints Principales

#### Chat y Consultas

```bash
# Enviar consulta individual
POST /api/chat/query
{
  "query": "Â¿CÃ³mo tramito una licencia?",
  "provider": "ollama",
  "use_rag": true,
  "rag_k": 5
}

# Comparar respuestas de modelos
POST /api/chat/compare
{
  "query": "Â¿QuÃ© es una declaraciÃ³n responsable?",
  "use_rag": true
}
```

#### BÃºsqueda RAG

```bash
# BÃºsqueda semÃ¡ntica
POST /api/rag/search
{
  "query": "licencia de apertura",
  "k": 10,
  "threshold": 0.7
}
```

#### Sistema

```bash
# Estado del sistema
GET /api/status

# EstadÃ­sticas completas
GET /api/stats

# ConfiguraciÃ³n de modelos
GET /api/config/models
```

### Ejemplo de Respuesta

```json
{
  "success": true,
  "content": "Para tramitar una licencia de obra necesitas...",
  "model": {
    "name": "llama3.2:3b",
    "type": "local",
    "response_time": 2.34,
    "tokens_used": 156
  },
  "rag": {
    "enabled": true,
    "sources": [
      {
        "source_path": "procedimientos.pdf",
        "content": "Las licencias de obra se tramitan...",
        "section_title": "CapÃ­tulo 3: Licencias"
      }
    ]
  }
}
```

## ðŸ›¡ï¸ Seguridad y Cumplimiento

### Cumplimiento Normativo

- **ENS** (Esquema Nacional de Seguridad): ConfiguraciÃ³n de logs y trazabilidad
- **CCN-TEC 014**: Recomendaciones para sistemas con IA
- **RGPD**: ProtecciÃ³n de datos personales

### CaracterÃ­sticas de Seguridad

- **Modelos Locales**: Datos sensibles no salen del servidor
- **Rate Limiting**: ProtecciÃ³n contra abuso de API
- **SanitizaciÃ³n**: ValidaciÃ³n de entrada de usuarios
- **Logging**: Registro completo de actividad
- **Headers de Seguridad**: ProtecciÃ³n contra XSS, clickjacking

### ConfiguraciÃ³n de Seguridad

```yaml
security:
  rate_limit_per_minute: 60
  max_query_length: 1000
  sanitize_inputs: true
  log_failed_requests: true
  
  security_headers:
    x_content_type_options: "nosniff"
    x_frame_options: "DENY"
```

## ðŸ”§ Desarrollo

### Configurar Entorno de Desarrollo

```bash
# Instalar dependencias de desarrollo
pip install -r requirements-dev.txt

# Configurar pre-commit hooks
pre-commit install

# Ejecutar tests
pytest tests/

# Linting y formateo
black app/
ruff check app/
```

### Estructura de Tests

```bash
tests/
â”œâ”€â”€ unit/                    # Tests unitarios
â”‚   â”œâ”€â”€ test_config.py      # Tests de configuraciÃ³n
â”‚   â”œâ”€â”€ test_models.py      # Tests de modelos
â”‚   â””â”€â”€ test_services.py    # Tests de servicios
â”œâ”€â”€ integration/            # Tests de integraciÃ³n
â”‚   â”œâ”€â”€ test_api.py        # Tests de API
â”‚   â””â”€â”€ test_rag.py        # Tests de sistema RAG
â””â”€â”€ fixtures/              # Datos de prueba
```

### Contribuir

1. Fork del repositorio
2. Crear rama de feature: `git checkout -b feature/nueva-funcionalidad`
3. Commit cambios: `git commit -m 'AÃ±adir nueva funcionalidad'`
4. Push a la rama: `git push origin feature/nueva-funcionalidad`
5. Crear Pull Request

## ðŸ“¦ Deployment

### Desarrollo Local

```bash
# Ejecutar en modo desarrollo
python run.py
```

### Docker

```bash
# Construir imagen
docker build -t prototipo_chatbot .

# Ejecutar contenedor
docker run -p 5000:5000 -v $(pwd)/data:/app/data prototipo_chatbot
```

### Cloud Deployment

#### Azure App Service

```bash
# Configurar Azure CLI
az login

# Crear App Service
az webapp create --resource-group myResourceGroup \
                 --plan myAppServicePlan \
                 --name prototipo-chatbot \
                 --runtime "PYTHON|3.9"

# Deploy
az webapp deploy --resource-group myResourceGroup \
                 --name prototipo-chatbot \
                 --src-path .
```

#### AWS Elastic Beanstalk

```bash
# Instalar EB CLI
pip install awsebcli

# Inicializar aplicaciÃ³n
eb init

# Crear entorno y deploy
eb create production
eb deploy
```

#### Google Cloud Run

```bash
# Construir imagen
gcloud builds submit --tag gcr.io/PROJECT_ID/prototipo_chatbot

# Deploy
gcloud run deploy --image gcr.io/PROJECT_ID/prototipo_chatbot \
                  --platform managed \
                  --region europe-west1
```

## ðŸ“ˆ Monitoreo y MÃ©tricas

### MÃ©tricas Disponibles

- **Rendimiento**: Tiempo de respuesta, throughput
- **Uso**: Consultas por usuario, modelos mÃ¡s usados
- **Calidad**: Tasa de Ã©xito, relevancia RAG
- **Sistema**: CPU, memoria, almacenamiento

### Logs

```bash
# Logs de aplicaciÃ³n
tail -f logs/prototipo_chatbot.log

# Logs de mÃ©tricas
tail -f logs/metrics.log

# Filtrar por nivel
grep "ERROR" logs/prototipo_chatbot.log
```

### Alertas

Configurar alertas para:
- Tiempo de respuesta > 5 segundos
- Tasa de error > 5%
- Uso de memoria > 85%
- Espacio en disco < 10%

## ðŸ” Troubleshooting

### Problemas Comunes

**Error: "Ollama no disponible"**
```bash
# Verificar que Ollama estÃ© ejecutÃ¡ndose
ollama list
curl http://localhost:11434/api/tags

# Reiniciar Ollama si es necesario
ollama serve
```

**Error: "Vector store vacÃ­o"**
```bash
# Verificar documentos en directorio
ls data/documents/

# Ejecutar ingesta manual
python -c "
from app.services.ingestion import process_documents
from app.services.rag import add_documents_to_rag
chunks = process_documents(['data/documents/'])
add_documents_to_rag(chunks)
"
```

**Error de memoria con embeddings**
```bash
# Reducir batch_size en config/settings.yaml
models:
  embedding:
    batch_size: 16  # Reducir de 32 a 16
```

### Logs de Debug

```bash
# Activar logging debug
export LOG_LEVEL=DEBUG
python run.py

# Ver logs especÃ­ficos
grep "rag_service" logs/prototipo_chatbot.log
grep "llm_service" logs/prototipo_chatbot.log
```

## ðŸ“š DocumentaciÃ³n Adicional

- **[GuÃ­a de Usuario](docs/user_guide.md)**: Uso completo del sistema
- **[GuÃ­a de Administrador](docs/admin_guide.md)**: ConfiguraciÃ³n y mantenimiento
- **[API Reference](docs/api_reference.md)**: DocumentaciÃ³n completa de API
- **[Arquitectura](docs/architecture.md)**: Detalles tÃ©cnicos del sistema
- **[Seguridad](docs/security.md)**: GuÃ­as de seguridad y cumplimiento

## ðŸŽ“ InformaciÃ³n del TFM

### Contexto AcadÃ©mico

- **Universidad**: Universitat Jaume I
- **MÃ¡ster**: Sistemas Inteligentes
- **Especialidad**: InteracciÃ³n Avanzada y GestiÃ³n del Conocimiento
- **Autor**: Vicente Caruncho Ramos
- **Tutor**: Rafael Berlanga Llavori
- **AÃ±o**: 2025

### Objetivos del TFM

1. **DiseÃ±ar** arquitectura RAG para administraciones locales
2. **Implementar** sistema de comparaciÃ³n de modelos LLM
3. **Evaluar** rendimiento de modelos locales vs. comerciales
4. **Garantizar** cumplimiento de normativas de seguridad
5. **Demostrar** viabilidad tÃ©cnica y funcional

### Resultados Esperados

- Sistema funcional de chatbot RAG
- Comparativa tÃ©cnica de modelos LLM
- MÃ©tricas de rendimiento y calidad
- DocumentaciÃ³n de buenas prÃ¡cticas
- Propuesta de mejoras futuras

## ðŸ¤ Colaboradores

- **Vicente Caruncho Ramos** - Desarrollo principal
- **Rafael Berlanga Llavori** - Tutor acadÃ©mico

## ðŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo [LICENSE](LICENSE) para detalles.

## ðŸ™ Agradecimientos

- Universitat Jaume I por el soporte acadÃ©mico
- Comunidad de desarrollo de LLMs y RAG
- Proyectos open source utilizados:
  - [Ollama](https://ollama.ai/) - Modelos locales
  - [sentence-transformers](https://www.sbert.net/) - Embeddings
  - [FAISS](https://faiss.ai/) - BÃºsqueda vectorial
  - [Flask](https://flask.palletsprojects.com/) - Framework web

---

**Prototipo_chatbot** - TFM 2025 - Vicente Caruncho Ramos

Para mÃ¡s informaciÃ³n: [vicente.caruncho@edu.uji.es](mailto:vicente.caruncho@edu.uji.es)