#!/usr/bin/env python3
"""
Script de Setup AutomÃ¡tico para Ollama - CORREGIDO
TFM Vicente Caruncho - Sistemas Inteligentes UJI
"""

import os
import sys
import subprocess
import requests
import time
from pathlib import Path

def check_ollama_installed():
    """Verificar si Ollama estÃ¡ instalado"""
    try:
        result = subprocess.run(['ollama', '--version'], 
                               capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            version = result.stdout.strip()
            print(f"âœ… Ollama encontrado: {version}")
            return True
        else:
            print("âŒ Ollama no estÃ¡ instalado o no estÃ¡ en PATH")
            return False
    except (subprocess.TimeoutExpired, FileNotFoundError):
        print("âŒ Ollama no estÃ¡ instalado")
        return False

def check_ollama_running():
    """Verificar si el servidor Ollama estÃ¡ ejecutÃ¡ndose"""
    try:
        response = requests.get('http://localhost:11434/api/tags', timeout=5)
        if response.status_code == 200:
            print("âœ… Servidor Ollama ejecutÃ¡ndose en localhost:11434")
            return True
        else:
            print(f"âŒ Servidor Ollama no responde (status: {response.status_code})")
            return False
    except requests.exceptions.RequestException:
        print("âŒ Servidor Ollama no estÃ¡ ejecutÃ¡ndose")
        return False

def get_installed_models():
    """Obtener lista de modelos instalados"""
    try:
        response = requests.get('http://localhost:11434/api/tags', timeout=10)
        if response.status_code == 200:
            data = response.json()
            models = [model['name'] for model in data.get('models', [])]
            return models
        return []
    except Exception as e:
        print(f"âŒ Error obteniendo modelos: {e}")
        return []

def quick_test():
    """Prueba rÃ¡pida para verificar que todo funciona"""
    print("\nâš¡ PRUEBA RÃPIDA DEL SISTEMA")
    print("=" * 30)
    
    try:
        # Test 1: Servidor Ollama
        if not check_ollama_running():
            print("âŒ Servidor Ollama no estÃ¡ ejecutÃ¡ndose")
            return False
        
        # Test 2: Modelos disponibles
        models = get_installed_models()
        if not models:
            print("âŒ No hay modelos instalados")
            return False
        
        print(f"âœ… {len(models)} modelos disponibles")
        for model in models[:3]:  # Mostrar primeros 3
            print(f"   - {model}")
        
        # Test 3: GeneraciÃ³n rÃ¡pida
        test_model = models[0]
        print(f"ğŸ”„ Probando generaciÃ³n con {test_model}...")
        
        payload = {
            "model": test_model,
            "prompt": "Hola",
            "stream": False,
            "options": {"max_tokens": 10}
        }
        
        response = requests.post(
            'http://localhost:11434/api/generate',
            json=payload,
            timeout=15
        )
        
        if response.status_code == 200:
            data = response.json()
            generated_text = data.get('response', '').strip()
            print(f"âœ… GeneraciÃ³n exitosa: {generated_text[:50]}...")
            return True
        else:
            print(f"âŒ Error en generaciÃ³n: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ Error en prueba rÃ¡pida: {e}")
        return False

def main():
    """FunciÃ³n principal del setup"""
    print("ğŸ“ TFM VICENTE CARUNCHO - SETUP OLLAMA")
    print("ğŸ›ï¸ Prototipo Chatbot RAG para Administraciones Locales")
    print("ğŸ¤– ConfiguraciÃ³n de Modelos de Lenguaje Locales")
    print("=" * 60)
    
    # Solo verificaciÃ³n bÃ¡sica para empezar
    print("\nğŸ” Verificando instalaciÃ³n de Ollama...")
    if not check_ollama_installed():
        print("âŒ Ollama no estÃ¡ instalado")
        print("ğŸ’¡ Descarga Ollama desde: https://ollama.ai/download")
        print("ğŸ’¡ Instala y vuelve a ejecutar este script")
        return 1
    
    print("\nğŸ–¥ï¸ Verificando servidor Ollama...")
    if not check_ollama_running():
        print("âŒ Servidor Ollama no estÃ¡ ejecutÃ¡ndose")
        print("ğŸ’¡ Intenta ejecutar: ollama serve")
        print("ğŸ’¡ O reinicia Ollama desde el menÃº de Windows")
        return 1
    
    print("\nğŸ“‹ Verificando modelos instalados...")
    models = get_installed_models()
    if models:
        print(f"âœ… {len(models)} modelos disponibles:")
        for model in models:
            print(f"   âœ… {model}")
    else:
        print("âš ï¸ No hay modelos instalados")
        print("ğŸ’¡ Para instalar el modelo principal: ollama pull llama3.2:3b")
        return 1
    
    print("\nâœ… Â¡Setup bÃ¡sico completado!")
    print("ğŸš€ Ollama estÃ¡ funcionando correctamente")
    
    return 0

if __name__ == "__main__":
    try:
        # Verificar si se solicita prueba rÃ¡pida
        if len(sys.argv) > 1 and sys.argv[1] == '--test':
            if quick_test():
                print("ğŸ‰ Sistema funcionando correctamente")
                sys.exit(0)
            else:
                print("âŒ Sistema requiere configuraciÃ³n")
                sys.exit(1)
        else:
            # Setup completo
            exit_code = main()
            sys.exit(exit_code)
            
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ Setup cancelado por el usuario")
        sys.exit(130)
    except Exception as e:
        print(f"\nğŸ’¥ Error inesperado: {e}")
        sys.exit(1)