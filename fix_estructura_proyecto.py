#!/usr/bin/env python3
"""
Script de CorrecciÃ³n - Estructura Proyecto TFM
Prototipo_chatbot - Vicente Caruncho Ramos
Universitat Jaume I - Sistemas Inteligentes

Corrige la estructura de mÃ³dulos basÃ¡ndose en lo que ya existe
"""

import os
import sys
from pathlib import Path
import shutil

def print_header(title: str):
    """Imprimir cabecera"""
    print("\n" + "=" * 80)
    print(f"ğŸ”§ {title}")
    print("=" * 80)

def analyze_current_structure():
    """Analizar estructura actual del proyecto"""
    print_header("ANÃLISIS DE ESTRUCTURA ACTUAL")
    
    current_dir = Path.cwd()
    print(f"ğŸ“ Directorio de trabajo: {current_dir}")
    
    # Buscar archivos Python existentes
    python_files = list(current_dir.rglob("*.py"))
    
    print(f"\nğŸ“„ Archivos Python encontrados: {len(python_files)}")
    
    # Categorizar archivos
    structure = {
        'core': [],
        'services': [],
        'models': [], 
        'routes': [],
        'tests': [],
        'scripts': []
    }
    
    for file_path in python_files:
        rel_path = str(file_path.relative_to(current_dir))
        
        if 'embedding' in rel_path.lower():
            structure['services'].append(rel_path)
        elif 'vector' in rel_path.lower() or 'faiss' in rel_path.lower() or 'chroma' in rel_path.lower():
            structure['services'].append(rel_path)
        elif 'config' in rel_path.lower() or 'logger' in rel_path.lower():
            structure['core'].append(rel_path)
        elif 'model' in rel_path.lower() or 'document' in rel_path.lower():
            structure['models'].append(rel_path)
        elif 'route' in rel_path.lower() or 'api' in rel_path.lower():
            structure['routes'].append(rel_path)
        elif 'test' in rel_path.lower():
            structure['tests'].append(rel_path)
        else:
            structure['scripts'].append(rel_path)
    
    # Mostrar estructura encontrada
    for category, files in structure.items():
        if files:
            print(f"\nğŸ“‚ {category.upper()}:")
            for file in files:
                print(f"   ğŸ“„ {file}")
    
    return structure

def create_missing_modules():
    """Crear mÃ³dulos faltantes basÃ¡ndose en lo que ya funciona"""
    print_header("CREANDO MÃ“DULOS FALTANTES")
    
    current_dir = Path.cwd()
    
    # 1. Crear estructura de directorios
    directories = [
        "app",
        "app/core",
        "app/models", 
        "app/services",
        "app/services/rag",
        "app/services/ingestion",
        "app/services/llm",
        "app/routes",
        "app/utils"
    ]
    
    for dir_path in directories:
        full_path = current_dir / dir_path
        full_path.mkdir(parents=True, exist_ok=True)
        print(f"   ğŸ“ {dir_path}")
    
    # 2. Crear archivos __init__.py
    init_files = [
        "app/__init__.py",
        "app/core/__init__.py",
        "app/models/__init__.py",
        "app/services/__init__.py", 
        "app/services/rag/__init__.py",
        "app/services/ingestion/__init__.py",
        "app/services/llm/__init__.py",
        "app/routes/__init__.py",
        "app/utils/__init__.py"
    ]
    
    for init_file in init_files:
        full_path = current_dir / init_file
        if not full_path.exists():
            full_path.write_text('"""MÃ³dulo del Prototipo_chatbot TFM"""\n')
            print(f"   âœ… {init_file}")
    
    return True

def create_document_model():
    """Crear modelo DocumentChunk faltante"""
    print_header("CREANDO MODELO DOCUMENTCHUNK")
    
    document_model_code = '''"""
Modelos de documentos para el sistema RAG
Prototipo_chatbot - TFM Vicente Caruncho Ramos
"""

from dataclasses import dataclass, field
from typing import Optional, Dict, Any, List
from datetime import datetime
import numpy as np

@dataclass
class DocumentMetadata:
    """Metadatos de un documento"""
    source_path: str
    source_type: str = "unknown"  # document, web, api, database
    file_type: str = ""
    size_bytes: int = 0
    created_at: datetime = field(default_factory=datetime.now)
    processed_at: datetime = field(default_factory=datetime.now)
    checksum: str = ""
    title: str = ""
    author: str = ""
    language: str = "es"
    extra: Dict[str, Any] = field(default_factory=dict)

@dataclass  
class DocumentChunk:
    """Fragmento de documento con embedding"""
    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    id: str = ""
    chunk_index: int = 0
    chunk_size: int = 0
    start_char: int = 0
    end_char: int = 0
    embedding: Optional[np.ndarray] = None
    
    def __post_init__(self):
        """Procesar despuÃ©s de inicializaciÃ³n"""
        if not self.id:
            import hashlib
            content_hash = hashlib.md5(self.content.encode()).hexdigest()[:8]
            self.id = f"chunk_{content_hash}_{self.chunk_index}"
        
        if not self.chunk_size:
            self.chunk_size = len(self.content)
        
        if not self.end_char and self.start_char:
            self.end_char = self.start_char + self.chunk_size

@dataclass
class SearchResult:
    """Resultado de bÃºsqueda semÃ¡ntica"""
    chunk: DocumentChunk
    score: float
    rank: int = 0
    
@dataclass
class RAGResponse:
    """Respuesta del sistema RAG"""
    query: str
    answer: str
    sources: List[DocumentChunk]
    confidence: float = 0.0
    processing_time: float = 0.0
    model_used: str = ""
    timestamp: datetime = field(default_factory=datetime.now)
'''
    
    # Escribir archivo
    model_file = Path("app/models/document.py")
    model_file.write_text(document_model_code)
    print(f"   âœ… Creado: {model_file}")
    
    # Actualizar __init__.py
    init_file = Path("app/models/__init__.py")
    init_content = '''"""Modelos del sistema RAG"""

from .document import DocumentChunk, DocumentMetadata, SearchResult, RAGResponse

__all__ = [
    "DocumentChunk",
    "DocumentMetadata", 
    "SearchResult",
    "RAGResponse"
]
'''
    init_file.write_text(init_content)
    print(f"   âœ… Actualizado: {init_file}")
    
    return True

def create_data_ingestion_service():
    """Crear servicio de ingesta de datos faltante"""
    print_header("CREANDO SERVICIO DE INGESTA")
    
    ingestion_code = '''"""
Servicio de ingesta de datos multimodal
Prototipo_chatbot - TFM Vicente Caruncho Ramos
"""

import os
import tempfile
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

from app.models.document import DocumentChunk, DocumentMetadata

class DataIngestionService:
    """Servicio de ingesta multimodal"""
    
    def __init__(self):
        self.supported_formats = ['.txt', '.pdf', '.docx', '.html']
        self.chunk_size = 1000
        self.chunk_overlap = 200
    
    def ingest_text_file(self, file_path: str) -> List[DocumentChunk]:
        """Ingerir archivo de texto plano"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Crear metadatos
            file_stat = os.stat(file_path)
            metadata = DocumentMetadata(
                source_path=file_path,
                source_type="document",
                file_type=Path(file_path).suffix,
                size_bytes=file_stat.st_size,
                created_at=datetime.fromtimestamp(file_stat.st_ctime),
                processed_at=datetime.now()
            )
            
            # Fragmentar contenido
            chunks = self._chunk_text(content, metadata.extra)
            
            return chunks
            
        except Exception as e:
            print(f"Error procesando {file_path}: {e}")
            return []
    
    def ingest_pdf(self, file_path: str) -> List[DocumentChunk]:
        """Simular ingesta PDF (requiere PyPDF2)"""
        # SimulaciÃ³n para evitar dependencias
        mock_content = f"Contenido simulado extraÃ­do de {Path(file_path).name}"
        
        chunks = [
            DocumentChunk(
                content=mock_content,
                metadata={
                    'source': file_path,
                    'page': 1,
                    'type': 'pdf_simulation'
                }
            )
        ]
        
        return chunks
    
    def ingest_docx(self, file_path: str) -> List[DocumentChunk]:
        """Simular ingesta DOCX (requiere python-docx)"""
        mock_content = f"Contenido simulado extraÃ­do de {Path(file_path).name}"
        
        chunks = [
            DocumentChunk(
                content=mock_content,
                metadata={
                    'source': file_path,
                    'type': 'docx_simulation'
                }
            )
        ]
        
        return chunks
    
    def ingest_url(self, url: str) -> List[DocumentChunk]:
        """Simular ingesta web (requiere beautifulsoup4)"""
        mock_content = f"Contenido web simulado de {url}"
        
        chunks = [
            DocumentChunk(
                content=mock_content,
                metadata={
                    'source': url,
                    'type': 'web_simulation',
                    'scraped_at': datetime.now().isoformat()
                }
            )
        ]
        
        return chunks
    
    def ingest_api(self, endpoint: str, auth_token: str = None) -> List[DocumentChunk]:
        """Simular ingesta API"""
        mock_content = f"Datos simulados de API {endpoint}"
        
        chunks = [
            DocumentChunk(
                content=mock_content,
                metadata={
                    'source': endpoint,
                    'type': 'api_simulation',
                    'retrieved_at': datetime.now().isoformat()
                }
            )
        ]
        
        return chunks
    
    def _chunk_text(self, text: str, metadata: Dict[str, Any]) -> List[DocumentChunk]:
        """Fragmentar texto en chunks"""
        chunks = []
        start_pos = 0
        chunk_index = 0
        
        while start_pos < len(text):
            end_pos = min(start_pos + self.chunk_size, len(text))
            
            # Buscar final de pÃ¡rrafo/oraciÃ³n cerca del lÃ­mite
            if end_pos < len(text):
                # Buscar punto seguido de espacio o salto de lÃ­nea
                for i in range(end_pos, max(start_pos, end_pos - 100), -1):
                    if text[i] in '.!?' and i + 1 < len(text) and text[i + 1] in ' \\n':
                        end_pos = i + 1
                        break
            
            chunk_content = text[start_pos:end_pos].strip()
            
            if chunk_content:
                chunk = DocumentChunk(
                    content=chunk_content,
                    metadata=metadata.copy(),
                    chunk_index=chunk_index,
                    start_char=start_pos,
                    end_char=end_pos
                )
                chunks.append(chunk)
                chunk_index += 1
            
            # Siguiente chunk con overlap
            start_pos = max(start_pos + 1, end_pos - self.chunk_overlap)
        
        return chunks
'''
    
    # Crear directorio si no existe
    ingestion_dir = Path("app/services/ingestion")
    ingestion_dir.mkdir(parents=True, exist_ok=True)
    
    # Escribir archivo
    ingestion_file = ingestion_dir / "data_ingestion.py"
    ingestion_file.write_text(ingestion_code)
    print(f"   âœ… Creado: {ingestion_file}")
    
    # Crear __init__.py para ingestion
    init_file = ingestion_dir / "__init__.py"
    init_content = '''"""Servicios de ingesta de datos"""

from .data_ingestion import DataIngestionService

__all__ = ["DataIngestionService"]
'''
    init_file.write_text(init_content)
    print(f"   âœ… Actualizado: {init_file}")
    
    return True

def create_llm_service():
    """Crear servicio LLM faltante"""
    print_header("CREANDO SERVICIO LLM")
    
    llm_code = '''"""
Servicio de modelos de lenguaje (LLM)
Prototipo_chatbot - TFM Vicente Caruncho Ramos
"""

import os
import time
import requests
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime

from app.models.document import DocumentChunk

@dataclass
class LLMResponse:
    """Respuesta de modelo de lenguaje"""
    content: str
    model_used: str
    processing_time: float
    tokens_used: int = 0
    estimated_cost: float = 0.0
    sources: List[str] = None
    
    def __post_init__(self):
        if self.sources is None:
            self.sources = []

class LLMService:
    """Servicio de gestiÃ³n de modelos de lenguaje"""
    
    def __init__(self):
        self.ollama_base_url = "http://localhost:11434"
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        
    def test_ollama_connection(self) -> bool:
        """Probar conexiÃ³n con Ollama"""
        try:
            response = requests.get(f"{self.ollama_base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def test_openai_connection(self) -> bool:
        """Probar conexiÃ³n con OpenAI"""
        return bool(self.openai_api_key and self.openai_api_key.startswith("sk-"))
    
    def get_ollama_models(self) -> List[str]:
        """Obtener modelos Ollama disponibles"""
        try:
            if not self.test_ollama_connection():
                return []
            
            response = requests.get(f"{self.ollama_base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models_data = response.json()
                return [model['name'] for model in models_data.get('models', [])]
            return []
        except:
            return []
    
    def get_openai_models(self) -> List[str]:
        """Obtener modelos OpenAI disponibles"""
        if not self.test_openai_connection():
            return []
        
        # Lista de modelos comunes (no requiere API call)
        return ['gpt-4o', 'gpt-4o-mini', 'gpt-3.5-turbo']
    
    def generate_local(self, query: str, context: List[DocumentChunk], 
                      model: str = "llama3.2:3b") -> LLMResponse:
        """Generar respuesta con modelo local (Ollama)"""
        start_time = time.time()
        
        try:
            # Construir prompt con contexto
            context_text = "\\n".join([chunk.content for chunk in context])
            prompt = f"""Contexto: {context_text}

Pregunta: {query}

Responde basÃ¡ndote Ãºnicamente en el contexto proporcionado."""

            # PeticiÃ³n a Ollama
            response = requests.post(
                f"{self.ollama_base_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,
                        "num_predict": 200  # Cambio de max_tokens a num_predict
                    }
                },
                timeout=30
            )
            
            processing_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                content = result.get('response', '').strip()
                
                return LLMResponse(
                    content=content,
                    model_used=model,
                    processing_time=processing_time,
                    tokens_used=0,  # Ollama no siempre reporta tokens
                    estimated_cost=0.0,  # Modelos locales son gratis
                    sources=[chunk.metadata.get('source', 'unknown') for chunk in context]
                )
            else:
                return LLMResponse(
                    content=f"Error HTTP {response.status_code}",
                    model_used=model,
                    processing_time=processing_time
                )
                
        except Exception as e:
            return LLMResponse(
                content=f"Error: {e}",
                model_used=model,
                processing_time=time.time() - start_time
            )
    
    def generate_openai(self, query: str, context: List[DocumentChunk],
                       model: str = "gpt-3.5-turbo") -> LLMResponse:
        """Generar respuesta con OpenAI"""
        if not self.test_openai_connection():
            return LLMResponse(
                content="Error: OpenAI API key no configurada",
                model_used=model,
                processing_time=0.0
            )
        
        start_time = time.time()
        
        try:
            import openai
            
            client = openai.OpenAI(api_key=self.openai_api_key)
            
            # Construir contexto
            context_text = "\\n".join([chunk.content for chunk in context])
            
            messages = [
                {
                    "role": "system",
                    "content": "Eres un asistente para administraciones locales. Responde basÃ¡ndote Ãºnicamente en el contexto proporcionado."
                },
                {
                    "role": "user", 
                    "content": f"Contexto: {context_text}\\n\\nPregunta: {query}"
                }
            ]
            
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=200,
                temperature=0.3
            )
            
            processing_time = time.time() - start_time
            
            content = response.choices[0].message.content.strip()
            
            # Calcular costo estimado
            prompt_tokens = response.usage.prompt_tokens
            completion_tokens = response.usage.completion_tokens
            
            if "gpt-4" in model.lower():
                cost = (prompt_tokens * 0.00003 + completion_tokens * 0.00006)
            else:  # gpt-3.5-turbo
                cost = (prompt_tokens * 0.000001 + completion_tokens * 0.000002)
            
            return LLMResponse(
                content=content,
                model_used=model,
                processing_time=processing_time,
                tokens_used=prompt_tokens + completion_tokens,
                estimated_cost=cost,
                sources=[chunk.metadata.get('source', 'unknown') for chunk in context]
            )
            
        except Exception as e:
            return LLMResponse(
                content=f"Error OpenAI: {e}",
                model_used=model,
                processing_time=time.time() - start_time
            )
'''
    
    # Crear directorio si no existe
    llm_dir = Path("app/services/llm")
    llm_dir.mkdir(parents=True, exist_ok=True)
    
    # Escribir archivo
    llm_file = llm_dir / "llm_service.py"
    llm_file.write_text(llm_code)
    print(f"   âœ… Creado: {llm_file}")
    
    # Crear __init__.py para llm
    init_file = llm_dir / "__init__.py"
    init_content = '''"""Servicios de modelos de lenguaje"""

from .llm_service import LLMService, LLMResponse

__all__ = ["LLMService", "LLMResponse"]
'''
    init_file.write_text(init_content)
    print(f"   âœ… Actualizado: {init_file}")
    
    return True

def update_main_services_init():
    """Actualizar __init__.py principal de services"""
    print_header("ACTUALIZANDO SERVICES __INIT__")
    
    services_init = Path("app/services/__init__.py")
    
    content = '''"""Servicios del sistema RAG"""

# Importaciones condicionales para evitar errores
try:
    from .rag.embeddings import embedding_service
except ImportError:
    embedding_service = None

try:
    from .rag.faiss_store import FaissVectorStore
except ImportError:
    FaissVectorStore = None

try:
    from .rag.chromadb_store import ChromaDBVectorStore
except ImportError:
    ChromaDBVectorStore = None

try:
    from .ingestion.data_ingestion import DataIngestionService
except ImportError:
    DataIngestionService = None

try:
    from .llm.llm_service import LLMService
except ImportError:
    LLMService = None

__all__ = [
    "embedding_service",
    "FaissVectorStore", 
    "ChromaDBVectorStore",
    "DataIngestionService",
    "LLMService"
]
'''
    
    services_init.write_text(content)
    print(f"   âœ… Actualizado: {services_init}")
    
    return True

def create_main_app_init():
    """Crear __init__.py principal de app"""
    print_header("ACTUALIZANDO APP __INIT__")
    
    app_init = Path("app/__init__.py")
    
    content = '''"""
Prototipo_chatbot - TFM Vicente Caruncho Ramos
Universitat Jaume I - Sistemas Inteligentes

Sistema RAG para Administraciones Locales
"""

__version__ = "1.0.0"
__author__ = "Vicente Caruncho Ramos"
__university__ = "Universitat Jaume I"
__project__ = "Prototipo de Chatbot RAG para Administraciones Locales"

# Importaciones principales
try:
    from .models.document import DocumentChunk, DocumentMetadata
except ImportError:
    DocumentChunk = None
    DocumentMetadata = None

try:
    from .services.rag.embeddings import embedding_service
except ImportError:
    embedding_service = None

def get_project_info():
    """InformaciÃ³n del proyecto"""
    return {
        "name": __project__,
        "version": __version__,
        "author": __author__,
        "university": __university__,
        "status": "TFM Development"
    }
'''
    
    app_init.write_text(content)
    print(f"   âœ… Actualizado: {app_init}")
    
    return True

def run_validation_test():
    """Ejecutar test de validaciÃ³n rÃ¡pida"""
    print_header("VALIDACIÃ“N RÃPIDA DE MÃ“DULOS")
    
    # Test imports bÃ¡sicos
    tests = [
        ("app.models.document", "DocumentChunk"),
        ("app.services.ingestion.data_ingestion", "DataIngestionService"), 
        ("app.services.llm.llm_service", "LLMService")
    ]
    
    successful_imports = 0
    
    for module_name, class_name in tests:
        try:
            # Intentar importar
            module = __import__(module_name, fromlist=[class_name])
            cls = getattr(module, class_name)
            
            print(f"   âœ… {module_name}.{class_name}")
            successful_imports += 1
            
        except ImportError as e:
            print(f"   âŒ {module_name}.{class_name} - ImportError: {e}")
        except AttributeError as e:
            print(f"   âŒ {module_name}.{class_name} - AttributeError: {e}")
        except Exception as e:
            print(f"   âŒ {module_name}.{class_name} - Error: {e}")
    
    print(f"\\nğŸ“Š Imports exitosos: {successful_imports}/{len(tests)}")
    
    if successful_imports == len(tests):
        print("âœ… Todos los mÃ³dulos funcionando correctamente")
        return True
    else:
        print("âš ï¸  Algunos mÃ³dulos necesitan ajustes")
        return False

def main():
    """FunciÃ³n principal de correcciÃ³n"""
    print_header("CORRECCIÃ“N DE ESTRUCTURA - PROYECTO TFM")
    print("ğŸ“‹ TFM: Prototipo de Chatbot RAG para Administraciones Locales")
    print("ğŸ‘¨â€ğŸ“ Autor: Vicente Caruncho Ramos") 
    print("ğŸ« Universidad: Universitat Jaume I - Sistemas Inteligentes")
    
    # Paso 1: Analizar estructura actual
    structure = analyze_current_structure()
    
    # Paso 2: Crear mÃ³dulos faltantes
    print("\\nğŸ”§ Iniciando correcciÃ³n...")
    
    steps = [
        ("Crear estructura de directorios", create_missing_modules),
        ("Crear modelo DocumentChunk", create_document_model),
        ("Crear servicio de ingesta", create_data_ingestion_service),
        ("Crear servicio LLM", create_llm_service),
        ("Actualizar services/__init__.py", update_main_services_init),
        ("Actualizar app/__init__.py", create_main_app_init)
    ]
    
    successful_steps = 0
    
    for step_name, step_function in steps:
        try:
            print(f"\\nğŸ”„ {step_name}...")
            if step_function():
                print(f"   âœ… {step_name} completado")
                successful_steps += 1
            else:
                print(f"   âŒ {step_name} fallÃ³")
        except Exception as e:
            print(f"   âŒ {step_name} - Error: {e}")
    
    # Paso 3: Validar correcciones
    print(f"\\nğŸ“Š Pasos completados: {successful_steps}/{len(steps)}")
    
    if successful_steps >= len(steps) - 1:  # Permitir 1 fallo
        print("\\nğŸ§ª Ejecutando validaciÃ³n final...")
        
        if run_validation_test():
            print_header("CORRECCIÃ“N COMPLETADA EXITOSAMENTE")
            print("âœ… Todos los mÃ³dulos creados y funcionando")
            print("ğŸš€ Ejecutar ahora: python ejecutar_fase4_rag_endtoend_validation.py")
        else:
            print_header("CORRECCIÃ“N PARCIALMENTE EXITOSA") 
            print("âš ï¸  Algunos mÃ³dulos necesitan ajustes manuales")
            print("ğŸ”§ Revisar imports en el validation script")
    else:
        print_header("CORRECCIÃ“N NECESITA TRABAJO ADICIONAL")
        print("âŒ Varios pasos fallaron - revisar errores anteriores")
    
    print("\\nğŸ“ Archivos creados en:")
    print("   ğŸ“ app/models/document.py")
    print("   ğŸ“ app/services/ingestion/data_ingestion.py") 
    print("   ğŸ“ app/services/llm/llm_service.py")

if __name__ == "__main__":
    """Punto de entrada"""
    try:
        main()
        
    except KeyboardInterrupt:
        print("\\n\\nâš ï¸ CORRECCIÃ“N INTERRUMPIDA POR USUARIO")
        
    except Exception as e:
        print(f"\\n\\nâŒ ERROR INESPERADO: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        print("\\nğŸ“š Para mÃ¡s informaciÃ³n consultar: README.md")
        print("ğŸ‘¨â€ğŸ“ TFM Vicente Caruncho Ramos - Universitat Jaume I")
        print("=" * 80)