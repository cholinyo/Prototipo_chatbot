#!/usr/bin/env python3
"""
Comparaci√≥n Acad√©mica: FAISS vs ChromaDB
Script para generar m√©tricas comparativas para TFM
Prototipo_chatbot - TFM Vicente Caruncho
"""

import sys
import os
import time
import json
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple
from app.models import DocumentChunk, DocumentMetadata


# A√±adir el directorio ra√≠z al path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def run_comparative_benchmark():
    """Ejecutar benchmark comparativo completo"""
    print("üî¨ BENCHMARKING ACAD√âMICO: FAISS vs ChromaDB")
    print("=" * 70)
    print("Prototipo_chatbot - TFM Vicente Caruncho Ramos")
    print("=" * 70)
    
    try:
        # Imports
        from app.services.rag.faiss_store import create_faiss_store, is_faiss_available
        from app.services.rag.chromadb_store import create_chromadb_store, is_chromadb_available
        from app.services.rag.embeddings import embedding_service
        from app.models import DocumentChunk, DocumentMetadata
        
        print("üì¶ M√≥dulos importados correctamente")
        
        # Verificar disponibilidad
        faiss_available = is_faiss_available()
        chromadb_available = is_chromadb_available()
        embedding_available = embedding_service.is_available()
        
        print(f"   ‚úÖ FAISS disponible: {faiss_available}")
        print(f"   ‚úÖ ChromaDB disponible: {chromadb_available}")
        print(f"   ‚úÖ EmbeddingService disponible: {embedding_available}")
        
        if not all([faiss_available, chromadb_available, embedding_available]):
            print("‚ùå No todos los servicios est√°n disponibles")
            return False
        
    except ImportError as e:
        print(f"‚ùå Error en imports: {e}")
        return False
    
    # Preparar datos de prueba
    print(f"\nüìä Preparando dataset de prueba...")
    
    # Dataset m√°s grande para comparaci√≥n meaningful
    test_documents = [
        "Administraci√≥n electr√≥nica y digitalizaci√≥n de servicios p√∫blicos municipales",
        "Procedimientos de licencias urban√≠sticas y de actividad en administraciones locales",
        "Gesti√≥n de expedientes administrativos y tramitaci√≥n de documentos oficiales",
        "Atenci√≥n ciudadana presencial y telem√°tica en ayuntamientos espa√±oles",
        "Normativa municipal sobre ordenanzas, reglamentos y disposiciones locales",
        "Hacienda local, presupuestos municipales y gesti√≥n financiera p√∫blica",
        "Planificaci√≥n urban√≠stica, PGOU y gesti√≥n del territorio municipal",
        "Servicios sociales municipales y pol√≠ticas de bienestar ciudadano",
        "Medio ambiente, sostenibilidad y gesti√≥n de residuos urbanos",
        "Transparencia, participaci√≥n ciudadana y gobierno abierto local",
        "Contrataci√≥n p√∫blica, licitaciones y procedimientos administrativos",
        "Personal funcionario, laborales y organizaci√≥n administrativa municipal",
        "Patrimonio municipal, bienes p√∫blicos y gesti√≥n del dominio p√∫blico",
        "Tributos locales, tasas municipales y gesti√≥n tributaria",
        "Seguridad ciudadana, polic√≠a local y protecci√≥n civil municipal",
        "Cultura, deportes y actividades de ocio en el √°mbito municipal",
        "Educaci√≥n municipal, escuelas infantiles y servicios educativos",
        "Sanidad p√∫blica local y servicios de salud comunitaria",
        "Transporte p√∫blico urbano y movilidad sostenible municipal",
        "Turismo local, promoci√≥n econ√≥mica y desarrollo territorial"
    ]
    
    # Queries de prueba representativas
    test_queries = [
        "licencias y permisos municipales",
        "servicios digitales administraci√≥n",
        "tramitaci√≥n expedientes ciudadanos",
        "normativa ordenanzas locales",
        "presupuestos hacienda municipal",
        "urbanismo planificaci√≥n territorial",
        "medio ambiente sostenibilidad",
        "atenci√≥n ciudadana servicios",
        "transparencia gobierno abierto",
        "personal funcionario municipal"
    ]
    
    print(f"   üìÑ Documentos de prueba: {len(test_documents)}")
    print(f"   üîç Queries de prueba: {len(test_queries)}")
    
    # Crear chunks de prueba
    metadata = DocumentMetadata(
        source_path="benchmark_documents.pdf",
        source_type="document", 
        file_type=".pdf",
        size_bytes=len(" ".join(test_documents)),
        created_at=datetime.now(),
        processed_at=datetime.now(),
        checksum="benchmark_checksum"
    )
    
    chunks = []
    for i, text in enumerate(test_documents):
        chunk = DocumentChunk(
            id=f"benchmark-chunk-{i+1:02d}",
            content=text,
            metadata=metadata,
            chunk_index=i,
            chunk_size=len(text),
            start_char=i*150,
            end_char=(i+1)*150
        )
        chunks.append(chunk)
    
    # Generar embeddings
    print(f"\nüß† Generando embeddings para {len(chunks)} documentos...")
    start_time = time.time()
    embedded_chunks = embedding_service.encode_documents(chunks)
    embedding_time = time.time() - start_time
    print(f"   ‚úÖ Embeddings generados en {embedding_time:.3f}s")
    
    # Generar embeddings para queries
    print(f"üîç Generando embeddings para {len(test_queries)} queries...")
    query_embeddings = []
    for query in test_queries:
        query_emb = embedding_service.encode_single_text(query)
        query_embeddings.append(query_emb)
    
    # Benchmarking FAISS
    print(f"\nüîµ BENCHMARKING FAISS")
    print("-" * 40)
    
    faiss_results = benchmark_vector_store(
        store_type="FAISS",
        store_factory=lambda: create_faiss_store(
            store_path="data/benchmark_faiss",
            dimension=384,
            index_type="IndexFlatL2"
        ),
        chunks=embedded_chunks,
        queries=test_queries,
        query_embeddings=query_embeddings
    )
    
    # Benchmarking ChromaDB
    print(f"\nüü¢ BENCHMARKING CHROMADB")
    print("-" * 40)
    
    chromadb_results = benchmark_vector_store(
        store_type="ChromaDB",
        store_factory=lambda: create_chromadb_store(
            store_path="data/benchmark_chromadb",
            collection_name="benchmark_collection",
            distance_function="cosine"
        ),
        chunks=embedded_chunks,
        queries=test_queries,
        query_embeddings=query_embeddings
    )
    
    # An√°lisis comparativo
    print(f"\nüìä AN√ÅLISIS COMPARATIVO")
    print("=" * 70)
    
    comparison = analyze_results(faiss_results, chromadb_results)
    
    # Generar reporte
    print(f"\nüìã GENERANDO REPORTE ACAD√âMICO...")
    report_path = generate_academic_report(faiss_results, chromadb_results, comparison)
    
    print(f"\nüéâ ¬°BENCHMARKING COMPLETADO!")
    print(f"üìÑ Reporte guardado en: {report_path}")
    print(f"üéì Datos listos para an√°lisis acad√©mico del TFM")
    
    return True


def benchmark_vector_store(store_type: str, store_factory, chunks: List[DocumentChunk], 
                          queries: List[str], query_embeddings: List[np.ndarray]) -> Dict[str, Any]:
    """Ejecutar benchmark completo de un vector store"""
    
    print(f"üèóÔ∏è  Inicializando {store_type}...")
    store = store_factory()
    
    if not store.is_available():
        print(f"   ‚ùå {store_type} no disponible")
        return {}
    
    print(f"   ‚úÖ {store_type} inicializado")
    
    # Limpiar store anterior
    store.clear()
    
    results = {
        "store_type": store_type,
        "timestamp": datetime.now().isoformat(),
        "dataset_size": len(chunks),
        "query_count": len(queries)
    }
    
    # Test 1: Inserci√≥n de documentos
    print(f"üì• Test inserci√≥n de documentos...")
    
    insertion_times = []
    batch_sizes = [5, 10, len(chunks)]  # Diferentes tama√±os de lote
    
    for batch_size in batch_sizes:
        if batch_size > len(chunks):
            continue
            
        # Limpiar para cada test
        store.clear()
        
        batch_chunks = chunks[:batch_size]
        
        start_time = time.time()
        success = store.add_documents(batch_chunks)
        insertion_time = time.time() - start_time
        
        if success:
            throughput = batch_size / insertion_time
            insertion_times.append({
                "batch_size": batch_size,
                "time": insertion_time,
                "throughput": throughput
            })
            print(f"   ‚úÖ {batch_size} docs en {insertion_time:.3f}s ({throughput:.1f} docs/s)")
        else:
            print(f"   ‚ùå Error insertando {batch_size} documentos")
    
    results["insertion_performance"] = insertion_times
    
    # Insertar todos los documentos para tests de b√∫squeda
    store.clear()
    full_insertion_start = time.time()
    store.add_documents(chunks)
    full_insertion_time = time.time() - full_insertion_start
    
    results["full_insertion_time"] = full_insertion_time
    results["full_insertion_throughput"] = len(chunks) / full_insertion_time
    
    # Test 2: B√∫squedas con diferentes valores de k
    print(f"üîç Test b√∫squedas con diferentes k...")
    
    search_performance = []
    k_values = [1, 3, 5, 10]
    
    for k in k_values:
        search_times = []
        result_counts = []
        
        for i, (query, query_emb) in enumerate(zip(queries, query_embeddings)):
            start_time = time.time()
            search_results = store.search(query_emb, k=k)
            search_time = time.time() - start_time
            
            search_times.append(search_time)
            result_counts.append(len(search_results))
        
        avg_search_time = np.mean(search_times)
        avg_results = np.mean(result_counts)
        
        search_performance.append({
            "k": k,
            "avg_search_time": avg_search_time,
            "avg_results_returned": avg_results,
            "queries_per_second": 1.0 / avg_search_time if avg_search_time > 0 else 0
        })
        
        print(f"   ‚úÖ k={k}: {avg_search_time:.3f}s avg, {avg_results:.1f} results avg")
    
    results["search_performance"] = search_performance
    
    # Test 3: B√∫squedas con filtros
    print(f"üéõÔ∏è  Test b√∫squedas con filtros...")
    
    filter_tests = [
        {"source_type": "document"},
        {"file_type": ".pdf"},
    ]
    
    filter_performance = []
    
    for filter_dict in filter_tests:
        filter_times = []
        
        for query_emb in query_embeddings[:5]:  # Solo primeras 5 queries
            start_time = time.time()
            filtered_results = store.search(query_emb, k=5, filters=filter_dict)
            filter_time = time.time() - start_time
            filter_times.append(filter_time)
        
        avg_filter_time = np.mean(filter_times)
        
        filter_performance.append({
            "filter": filter_dict,
            "avg_time": avg_filter_time,
            "queries_per_second": 1.0 / avg_filter_time if avg_filter_time > 0 else 0
        })
        
        print(f"   ‚úÖ Filter {filter_dict}: {avg_filter_time:.3f}s avg")
    
    results["filter_performance"] = filter_performance
    
    # Test 4: M√©tricas del sistema
    print(f"üìä Recopilando m√©tricas del sistema...")
    
    final_stats = store.get_stats()
    results["system_metrics"] = final_stats
    
    # M√©tricas espec√≠ficas seg√∫n el tipo
    if store_type == "FAISS":
        results["memory_usage"] = final_stats.get("index_size_mb", 0)
        results["storage_type"] = "In-memory + Files"
    elif store_type == "ChromaDB":
        results["disk_usage"] = final_stats.get("disk_usage_mb", 0)
        results["storage_type"] = "Persistent Database"
    
    print(f"   ‚úÖ M√©tricas recopiladas")
    
    return results


def analyze_results(faiss_results: Dict[str, Any], chromadb_results: Dict[str, Any]) -> Dict[str, Any]:
    """Analizar y comparar resultados"""
    
    analysis = {
        "comparison_timestamp": datetime.now().isoformat(),
        "dataset_info": {
            "documents": faiss_results.get("dataset_size", 0),
            "queries": faiss_results.get("query_count", 0),
            "embedding_dimension": 384
        }
    }
    
    # Comparaci√≥n de inserci√≥n
    faiss_insertion = faiss_results.get("full_insertion_throughput", 0)
    chromadb_insertion = chromadb_results.get("full_insertion_throughput", 0)
    
    analysis["insertion_comparison"] = {
        "faiss_throughput": faiss_insertion,
        "chromadb_throughput": chromadb_insertion,
        "winner": "FAISS" if faiss_insertion > chromadb_insertion else "ChromaDB",
        "improvement_factor": max(faiss_insertion, chromadb_insertion) / min(faiss_insertion, chromadb_insertion) if min(faiss_insertion, chromadb_insertion) > 0 else 0
    }
    
    # Comparaci√≥n de b√∫squeda (k=5)
    faiss_search_k5 = next((p for p in faiss_results.get("search_performance", []) if p["k"] == 5), {})
    chromadb_search_k5 = next((p for p in chromadb_results.get("search_performance", []) if p["k"] == 5), {})
    
    faiss_search_time = faiss_search_k5.get("avg_search_time", float('inf'))
    chromadb_search_time = chromadb_search_k5.get("avg_search_time", float('inf'))
    
    analysis["search_comparison"] = {
        "faiss_avg_time": faiss_search_time,
        "chromadb_avg_time": chromadb_search_time,
        "winner": "FAISS" if faiss_search_time < chromadb_search_time else "ChromaDB",
        "speedup_factor": max(faiss_search_time, chromadb_search_time) / min(faiss_search_time, chromadb_search_time) if min(faiss_search_time, chromadb_search_time) > 0 else 0
    }
    
    # Comparaci√≥n de almacenamiento
    faiss_storage = faiss_results.get("memory_usage", 0)
    chromadb_storage = chromadb_results.get("disk_usage", 0)
    
    analysis["storage_comparison"] = {
        "faiss_mb": faiss_storage,
        "chromadb_mb": chromadb_storage,
        "faiss_type": "Memory + Files",
        "chromadb_type": "Persistent DB"
    }
    
    # Recomendaciones
    recommendations = []
    
    if faiss_insertion > chromadb_insertion * 1.5:
        recommendations.append("FAISS superior para inserci√≥n masiva de datos")
    
    if faiss_search_time < chromadb_search_time * 0.5:
        recommendations.append("FAISS superior para b√∫squedas de alta velocidad")
    elif chromadb_search_time < faiss_search_time * 0.5:
        recommendations.append("ChromaDB superior para b√∫squedas de alta velocidad")
    
    if chromadb_storage > 0:
        recommendations.append("ChromaDB ofrece persistencia autom√°tica")
    
    recommendations.append("FAISS ideal para aplicaciones que priorizan velocidad")
    recommendations.append("ChromaDB ideal para aplicaciones que requieren persistencia")
    
    analysis["recommendations"] = recommendations
    
    # Mostrar an√°lisis
    print(f"\nüìä RESULTADOS COMPARATIVOS:")
    print(f"üì• Inserci√≥n:")
    print(f"   ‚Ä¢ FAISS: {faiss_insertion:.1f} docs/s")
    print(f"   ‚Ä¢ ChromaDB: {chromadb_insertion:.1f} docs/s")
    print(f"   ‚Ä¢ Ganador: {analysis['insertion_comparison']['winner']}")
    
    print(f"\nüîç B√∫squeda (k=5):")
    print(f"   ‚Ä¢ FAISS: {faiss_search_time:.3f}s promedio")
    print(f"   ‚Ä¢ ChromaDB: {chromadb_search_time:.3f}s promedio")
    print(f"   ‚Ä¢ Ganador: {analysis['search_comparison']['winner']}")
    
    print(f"\nüíæ Almacenamiento:")
    print(f"   ‚Ä¢ FAISS: {faiss_storage:.2f} MB (memoria + archivos)")
    print(f"   ‚Ä¢ ChromaDB: {chromadb_storage:.2f} MB (base de datos)")
    
    print(f"\nüí° Recomendaciones clave:")
    for rec in recommendations[:3]:
        print(f"   ‚Ä¢ {rec}")
    
    return analysis


def generate_academic_report(faiss_results: Dict[str, Any], chromadb_results: Dict[str, Any], 
                           analysis: Dict[str, Any]) -> str:
    """Generar reporte acad√©mico completo"""
    
    report_dir = Path("data/reports")
    report_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = report_dir / f"vector_stores_comparison_{timestamp}.json"
    
    # Compilar reporte completo
    full_report = {
        "metadata": {
            "title": "Comparaci√≥n Emp√≠rica FAISS vs ChromaDB",
            "subtitle": "An√°lisis de Rendimiento para Sistemas RAG",
            "author": "Vicente Caruncho Ramos",
            "tfm": "Prototipo de Chatbot Interno para Administraciones Locales",
            "university": "Universitat Jaume I",
            "date": datetime.now().isoformat(),
            "version": "1.0"
        },
        "methodology": {
            "dataset_size": analysis["dataset_info"]["documents"],
            "query_count": analysis["dataset_info"]["queries"],
            "embedding_model": "all-MiniLM-L6-v2",
            "embedding_dimension": analysis["dataset_info"]["embedding_dimension"],
            "test_categories": [
                "Inserci√≥n de documentos",
                "B√∫squedas por similitud",
                "Filtrado por metadatos", 
                "Uso de recursos"
            ]
        },
        "results": {
            "faiss": faiss_results,
            "chromadb": chromadb_results
        },
        "analysis": analysis,
        "conclusions": {
            "performance_winner": analysis["search_comparison"]["winner"],
            "throughput_winner": analysis["insertion_comparison"]["winner"],
            "use_case_recommendations": analysis["recommendations"]
        }
    }
    
    # Guardar reporte
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(full_report, f, indent=2, ensure_ascii=False, default=str)
    
    # Generar resumen en markdown
    md_path = report_dir / f"vector_stores_summary_{timestamp}.md"
    generate_markdown_summary(full_report, md_path)
    
    return str(report_path)


def generate_markdown_summary(report: Dict[str, Any], md_path: Path):
    """Generar resumen en Markdown para el TFM"""
    
    analysis = report["analysis"]
    
    markdown_content = f"""# Comparaci√≥n Emp√≠rica: FAISS vs ChromaDB

**Autor:** {report["metadata"]["author"]}  
**TFM:** {report["metadata"]["tfm"]}  
**Fecha:** {datetime.now().strftime("%d/%m/%Y")}

## Metodolog√≠a

- **Dataset:** {analysis["dataset_info"]["documents"]} documentos de administraci√≥n local
- **Queries:** {analysis["dataset_info"]["queries"]} consultas representativas
- **Modelo embeddings:** all-MiniLM-L6-v2 (384 dimensiones)
- **M√©tricas:** Latencia, throughput, uso de recursos

## Resultados Principales

### Rendimiento de Inserci√≥n
- **FAISS:** {analysis["insertion_comparison"]["faiss_throughput"]:.1f} docs/segundo
- **ChromaDB:** {analysis["insertion_comparison"]["chromadb_throughput"]:.1f} docs/segundo
- **Ganador:** {analysis["insertion_comparison"]["winner"]}

### Rendimiento de B√∫squeda
- **FAISS:** {analysis["search_comparison"]["faiss_avg_time"]:.3f}s promedio
- **ChromaDB:** {analysis["search_comparison"]["chromadb_avg_time"]:.3f}s promedio  
- **Ganador:** {analysis["search_comparison"]["winner"]}

### Almacenamiento
- **FAISS:** {analysis["storage_comparison"]["faiss_mb"]:.2f} MB ({analysis["storage_comparison"]["faiss_type"]})
- **ChromaDB:** {analysis["storage_comparison"]["chromadb_mb"]:.2f} MB ({analysis["storage_comparison"]["chromadb_type"]})

## Conclusiones para Administraciones Locales

### Recomendaciones de Uso
"""
    
    for i, rec in enumerate(analysis["recommendations"], 1):
        markdown_content += f"{i}. {rec}\n"
    
    markdown_content += f"""
### Implicaciones para el TFM

- **FAISS** se muestra superior para casos de uso que priorizan **velocidad de respuesta**
- **ChromaDB** ofrece ventajas en **persistencia** y **gesti√≥n de metadatos**
- Ambas tecnolog√≠as son **viables** para sistemas RAG en administraciones locales
- La elecci√≥n depende de los **requisitos espec√≠ficos** del caso de uso

### Trabajo Futuro

- Evaluaci√≥n con datasets m√°s grandes (>10,000 documentos)
- An√°lisis de calidad de resultados (relevancia, precisi√≥n)
- Pruebas con diferentes modelos de embeddings
- Evaluaci√≥n de escalabilidad en producci√≥n
"""
    
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write(markdown_content)


def cleanup_benchmark_data():
    """Limpiar datos de benchmark"""
    import shutil
    
    for dir_name in ["data/benchmark_faiss", "data/benchmark_chromadb"]:
        dir_path = Path(dir_name)
        if dir_path.exists():
            try:
                shutil.rmtree(dir_path)
                print(f"üßπ Limpiado: {dir_name}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Error limpiando {dir_name}: {e}")


if __name__ == "__main__":
    print("üéì Comparaci√≥n Acad√©mica Vector Stores - TFM")
    print("Vicente Caruncho Ramos - Sistemas Inteligentes")
    print("=" * 70)
    
    try:
        success = run_comparative_benchmark()
        
        if success:
            print(f"\nüéâ ¬°COMPARACI√ìN ACAD√âMICA COMPLETADA EXITOSAMENTE!")
            print(f"üìä Datos emp√≠ricos generados para an√°lisis TFM")
            print(f"üìà M√©tricas listas para memoria y defensa")
            cleanup_benchmark_data()
        else:
            print(f"\nüíî Error en comparaci√≥n acad√©mica")
            
    except KeyboardInterrupt:
        print(f"\n‚èπÔ∏è  Comparaci√≥n interrumpida por el usuario")
        cleanup_benchmark_data()
    except Exception as e:
        print(f"\nüí• Error inesperado: {e}")
        cleanup_benchmark_data()